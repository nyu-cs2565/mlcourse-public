\newcommand*{\BeamerSuffix}{-pres}
\newcommand*{\TransSuffix}{-slides}
% \documentclass{beamerswitch}
\documentclass{beamer}
\setbeamertemplate{navigation symbols}{}

\usepackage{movie15}
\usepackage{amssymb,amsmath}
\usepackage{hyperref}
\usepackage{natbib} 
\usepackage{pgf} 
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{bbm}

\usetheme{Madrid}   % clean, nice.  7/12 page numbers


\input{macros}
%\input{defs}
\newcommand{\paramVec}{\mathbf{\theta}}

%%% From defs
\newcommand{\bw}{\mathbf{w}}

%%%
%From Commands.tex

\newcommand{\eqdef}{\triangleq}
\newcommand{\EE}[1]{{\mathbb E}\left[#1\right]}
\newcommand{\EEX}[2]{{\mathbb E}_{#1}\left[#2\right]}
\newcommand{\Qopt}{Q^*}
\newcommand{\Vpi}{V^\pi}
\newcommand{\Qpi}{Q^\pi}
\newcommand{\Vopt}{V^*}
\newcommand{\Qhat}{\hat{Q}}
\newcommand{\Tpi}{T^\pi}
\newcommand{\Topt}{{T^*}}
\newcommand{\piopt}{{\pi^*}}
\newcommand{\argmin}{\mathop{\text{argmin}}}
\newcommand{\argmax}{\mathop{\text{argmax}}}
\newcommand{\States}{\mathcal{S}}
\newcommand{\Actions}{\mathcal{A}}
\newcommand{\PKernel}{\mathcal{P}}
\newcommand{\RKernel}{\mathcal{R}}
\newcommand{\SA}{\States\times\Actions}
\newcommand{\dx}{\mathrm{d}x}
\newcommand{\dy}{\mathrm{d}y}
\newcommand{\dz}{\mathrm{d}z}
\newcommand{\dmu}{\mathrm{d}\mu}
\newcommand{\dnu}{\mathrm{d}\nu}
\newcommand{\drho}{\mathrm{d}\rho}
\newcommand{\ds}{\mathrm{d}s}
\newcommand{\ra}{\rightarrow}
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}


\renewcommand{\high}{\textbf}
\title[CSC411 2019 Winter Lecture 22]{CSC 411: Introduction to Machine Learning}
\subtitle{CSC 411 Lecture 22: Reinforcement Learning II}
\author[UofT]{Mengye Ren and Matthew MacKay}
\institute[]{University of Toronto}
\date{}

\begin{document}
\begin{frame}
  \titlepage
\end{frame}
\setbeamercovered{invisible}

\begin{frame}\frametitle{MDP}\small
\begin{itemize}
\item Markov Decision Problem (MDP): tuple $(S,A,P,\gamma)$ where $P$ is
\[
P(s_{t+1}=s', r_{t+1}=r' | s_t = s, a_t = a)
\]
\onslide<2->\item Main assumption: Markovian dynamics and reward.
\onslide<3->\item Standard MDP problems:
\begin{enumerate}
\onslide<4->\item  \high{Planning}: given complete Markov decision problem as input, compute policy with optimal expected return
\begin{figure}
\includegraphics[width=0.7\linewidth]{Figures/rll6} 
\end{figure}
\end{enumerate}
\end{itemize}
\scriptsize [Pic: P. Abbeel]
\end{frame}

\begin{frame}\frametitle{Basic Problems}\small
\begin{itemize}
\item Markov Decision Problem (MDP): tuple $(S,A,P,\gamma)$ where $P$ is
\[
P(s_{t+1}=s', r_{t+1}=r' | s_t = s, a_t = a)
\]
\item Standard MDP problems:
\begin{enumerate}
\item  \high{Planning}: given complete Markov decision problem as input, compute policy with optimal expected return\\[1mm]
\item \high{Learning}: We don't know which states are good or what the actions do. We must try out the actions and states to learn what to do
\end{enumerate}
\end{itemize}
\vspace{28mm}
\scriptsize [P. Abbeel]
\end{frame}

\begin{frame}\frametitle{Example of Standard MDP Problem}\small
\begin{center}
\includegraphics[width=0.54\linewidth]{Figures/tic_tac_rl} 
\end{center}
\begin{enumerate}
\item  \high{Planning}: given complete Markov decision problem as input, compute policy with optimal expected return
\item \high{Learning}: Only have access to experience in the MDP, learn a near-optimal strategy
\end{enumerate}
%We will focus on learning, but discuss planning along the way
\end{frame}

\begin{frame}\frametitle{Example of Standard MDP Problem}\small
\begin{center}
\includegraphics[width=0.785\linewidth,trim=0 50 0 0,clip]{Figures/rll4} 
\end{center}
\begin{enumerate}
\item  \high{Planning}: given complete Markov decision problem as input, compute policy with optimal expected return
\item \high{Learning}: Only have access to experience in the MDP, learn a near-optimal strategy
\end{enumerate}
We will focus on learning, but discuss planning along the way
\end{frame}

\begin{frame}\frametitle{Exploration vs. Exploitation}\small
\begin{itemize}
\item If we knew how the world works (embodied in $P$), then the policy should be deterministic 
\begin{itemize}
\onslide<2->\item  just select optimal action in each state
\end{itemize}
\onslide<3->\item Reinforcement learning is like trial-and-error learning
\onslide<4->\item The agent should discover a good policy from its experiences of the environment
\onslide<4->\item Without losing too much reward along the way

\onslide<5->\item Since we do not have complete knowledge of the world, taking what appears to be the optimal action may prevent us from finding better states/actions
\onslide<6->\item Interesting trade-off: 
\begin{itemize}
\item immediate reward (\high{exploitation}) vs. gaining knowledge that might enable higher future reward (\high{exploration})
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Examples}\small
\begin{itemize}
\item Restaurant Selection
\begin{itemize}
\item \high{Exploitation}: Go to your favourite restaurant
\item \high{Exploration}: Try a new restaurant
\end{itemize}
\item Online Banner Advertisements
\begin{itemize}
\item \high{Exploitation}: Show the most successful advert
\item \high{Exploration}: Show a different advert
\end{itemize}
\item Oil Drilling
\begin{itemize}
\item \high{Exploitation}: Drill at the best known location
\item \high{Exploration}: Drill at a new location
\end{itemize}
\item Game Playing
\begin{itemize}
\item \high{Exploitation}: Play the move you believe is best
\item \high{Exploration}: Play an experimental move
\end{itemize}
\end{itemize}
\vspace{1mm}
\scriptsize [Slide credit: D. Silver]
\end{frame}

\begin{frame}\frametitle{Value function}\small
\begin{itemize}
\item The value function $V^\pi(s)$ assigns each state the expected reward 
\[
V^\pi(s)=\underset{a_{t},a_{t+i},s_{t+i}}{\mathbb{E}}\left[\sum_{i=1}^\infty\gamma^{i} r_{t+i} |s_t=s\right]
\]
\onslide<2->\item Usually not informative enough to make decisions.
\onslide<3->\item The $Q$-value $Q^{\pi}(s,a)$ is the expected reward of taking action $a$ in state $s$ and then continuing according to $\pi$.
\[
Q^\pi(s,a)=\underset{a_{t+i},s_{t+i}}{\mathbb{E}}\left[\sum_{i=1}^\infty\gamma^{i} r_{t+i} |s_t=s,a_t=a\right]
\]
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Bellman equations}\small
\begin{itemize}
    \item The foundation of many RL algorithms
    {\footnotesize
    \begin{align*}\footnotesize
    V^\pi(s)&=\underset{a_{t},a_{t+i},s_{t+i}}{\mathbb{E}}\left[\sum_{i=1}^\infty\gamma^{i} r_{t+i} |s_t=s\right] \\
    &=\underset{a_{t}}{\mathbb{E}}\left[r_{t+1} |s_t=s\right]  +\gamma \underset{a_{t},a_{t+i},s_{t+i}}{\mathbb{E}}\left[\sum_{i=1}^\infty\gamma^{i} r_{t+i+1} |s_t=s\right] \\
    & = \underset{a_{t}}{\mathbb{E}}\left[r_{t+1} |s_t=s\right] +\gamma \underset{s_{t+1}}{\mathbb{E}}\left[V^{\pi}(s_{t+1})|s_t=s\right] \\
    & = \sum_{a,r}P^{\pi}(a|s_t)p(r|a,s_t)\cdot r+\gamma \sum_{a,s'}P^{\pi}(a|s_t)p(s'|a,s_t)\cdot V^{\pi}(s')
    \end{align*}}
    \item Similar equation holds for $Q$
        {\footnotesize
        \begin{align*}\footnotesize
        &Q^\pi(s,a)=\underset{a_{t+i},s_{t+i}}{\mathbb{E}}\left[\sum_{i=1}^\infty\gamma^{i} r_{t+i} |s_t=s,a_t=a\right] \\
        & = \sum_{r}p(r|a,s_t)\cdot r+ \gamma\sum_{s'}p(s'|a,s_t)\cdot V^{\pi}(s') \\
        &=\sum_{r}p(r|a,s_t)\cdot r+ \gamma\sum_{a',s'}p(s'|a,s_t)p(a'|s')\cdot Q^{\pi}(s',a')
        \end{align*}}
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Solving Bellman equations}\small
\begin{itemize}
    \item The Bellman equations are a set of linear equations with a unique solution.
    % \onslide<2->\item Can solve fast(er) because the linear mapping is a contractive mapping.
    \onslide<2->\item This lets you know the quality of each state/action under your policy - \high{policy evaluation}.
    \onslide<3->\item You can improve by picking $\pi'(s)=\max_a Q^{\pi}(s,a)$ - \high{policy improvement}.
    \onslide<4->\item Can show the iterative policy evaluation and improvement converges to the optimal policy - \high{policy iteration}.
    \onslide<5->\item Are we done? % \onslide<7-> Why isn't this enough?
    \onslide<6->
    \begin{itemize}
        \item Need to know the model. Usually isn't known.
        \item Number of states is usually huge (how many unique states does a chess game have?) 
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Optimal Bellman equations}\small
\begin{itemize}
    \item First step is understand the Bellman equation for the optimal policy $\pi^*$
    \onslide<2->\item Under this policy $V^*(s)=\max_a Q^*(s,a)$
        {\footnotesize
        \begin{align*}\footnotesize
        V^*(s)& = \max_a\left[\mathbb{E}\left[r_{t+1} |s_t=s,a_t=a\right] +\gamma \underset{s_{t+1}}{\mathbb{E}}\left[V^{*}(s_{t+1})|s_t=s,a_t=a\right]\right] \\
        &= \max_a\left[\sum_{r}p(r|a,s_t)\cdot r+\gamma \sum_{s'}p(s'|a,s_t)\cdot V^*(s')\right] \\
        Q^*(s,a)&=\mathbb{E}\left[r_{t+1} |s_t=s,a_t=a\right]+ \gamma\underset{s_{t+1}}{\mathbb{E}}\left[\max_{a'}Q^{*}(s_{t+1},a')|s_t=s,a_t=a\right] \\
        &= \sum_{r}p(r|a,s_t)\cdot r+\gamma \sum_{s'}p(s'|a,s_t)\cdot\max_{a'} Q^*(s',a')
        \end{align*}}
    \onslide<3-> \item Set on nonlinear equations.
    \onslide<4-> \item Same issues as before.
\end{itemize}
\end{frame}


\begin{frame}\frametitle{Q-learning intuition}\small
\begin{itemize}
    \item Q-learning is a simple algorithm to find the optimal policy without knowing the model.
        \onslide<2->\item $Q^*$ is the unique solution to the optimal Bellman equation.
    \[
    Q^*(s,a)=\mathbb{E}\left[r_{t+1} |s_t=s,a_t=a\right]+ \gamma\underset{s_{t+1}}{\mathbb{E}}\left[\max_{a'}Q^{*}(s_{t+1},a')|s_t=s,a_t=a\right]
    \]
    \onslide<3->\item We don't know the model and don't want to update all states simultaneously.
    \onslide<4->\item Solution - given sample $s_t,a_t,r_{t+1},s_{t+1}$ from the environment update your $Q$-values so they are closer to satisfying the bellman equation.
    \begin{itemize}
        \item \high{off-policy} method: Samples don't have to be from the optimal policy.
    \end{itemize} 
    \item Samples need to be diverse enough to see everything - exploration.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Exploration vs exploitation}\small
\begin{itemize}
    \item Given $Q$-value the best thing we can do (given our limited knowledge) is to take $a=\arg\max_{a'}Q(s,a')$ - \high{exploitation}
    \onslide<2->\item How do we balance exploration with exploitation?
    \onslide<3->\item Simplest solution: $\epsilon$-greedy. 
    \begin{itemize}
        \item With probability $1-\epsilon$ pick $a=\arg\max_{a'}Q(s,a')$ (i.e. greedy)
        \item With probability $\epsilon$ pick any other action uniformly.
    \end{itemize} 
    \onslide<4->\item Another idea - softmax using $Q$ values
    \begin{itemize}
        \item With probability $1-\epsilon$ pick $a=\arg\max_{a'}Q(s,a')$ (i.e. greedy)
        \item With probability $\epsilon$ pick any other action with probability $\propto\exp(\beta Q(s,a))$.
    \end{itemize} 
    \onslide<5->\item Other fancier solutions exist, many leading methods use simple $\epsilon$-greedy sampling.
\end{itemize}
\end{frame}


\begin{frame}\frametitle{Q-learning algorithm}\small
\vspace{-0.5cm}
\begin{figure}
     \includegraphics[width=0.9\linewidth]{Figures/Qalgo}
 \end{figure}
\vspace{-0.5cm}
\begin{itemize}
    \onslide<2->\item Can prove convergence to the optimal $Q^*$ under mild conditions.
    \onslide<3->\item Update is equivalent to gradient descent on loss $||R+\gamma\max_a Q(S',a)-Q(s,a)||^2$.
    \item At optimal Q, the loss is 0.
    % \onslide<4->\item Why $L_2$ loss? Optimal solution is the mean which is what we are looking for!
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Bootstrapping}\small
\begin{itemize}
    \item Another way to think about Q-learning.
    \item $Q(s,a)$ is the expected reward, can use Monte-Carlo estimation.
    \onslide<2->\item Problem - you update only after the episode ends, can be very long (or infinite).
    \onslide<3->\item Q-learning solution - take only 1 step forward and estimate the future using our Q value - \high{bootstrapping}.
    \begin{itemize}
        \item "learn a guess from a guess"
    \end{itemize} 
    \onslide<4->\item Q-learning is just one algorithm in a family of algorithms that use this idea.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Function approximation}\small
\begin{itemize}
    \item Q-learning still scales badly with large state spaces, how many states does a chess game have? Need to save the full table!
    \onslide<2->\item Similar states, e.g. move all chess pieces two steps to the left, at treated as totally different.
    \onslide<3->\item Solution: Instead of $Q$ being a $S\times A$ table it is a parametrized function.
    \onslide<4-> \item Looking for function $\hat{Q}(s,a;\bw)\approx Q^*(s,a)$
    \begin{itemize}
        \onslide<5->\item Linear functions $Q(s,a;\bw)=\bw^T\phi(s,a)$.
        \item Neural network
    \end{itemize}
    \onslide<5->\item Hopefully can generalize to unseen states.
    \onslide<6->\item Problem: Each change to parameters changes all states/actions - can lead to instability.
    \onslide<7->\item For non-linear Q-learning can diverge.
\end{itemize}
\end{frame}


\begin{frame}\frametitle{Deep Q-learning}\small
\begin{itemize}
    \item We have a function approximator $Q(s,a;\theta)$, standard is neural net but doesn't have to be.
    \onslide<2->\item What is the objective that we are optimizing?
    \onslide<3->\item  We want to minimize $\mathbb{E}_{\rho}[||R+\gamma\max_{a'} Q(S',a')-Q(s,a)||^2]$
    \begin{itemize}
        \item $\rho$ is a distribution over states, depends on $\theta$!
    \end{itemize}
    \onslide<4->\item Two terms depend on $Q$, don't want to take gradients w.r. to $\gamma\max_aQ(S',a)$
    \onslide<5->\item We want to correct our previous estimation given the new information.
            \vspace{-0.2cm}
    \begin{figure}
        \includegraphics[width=0.85\linewidth]{Figures/Qalgo2}
        \vspace{-0.4cm}
        \caption{Take from:rll.berkeley.edu/deeprlcourse}
    \end{figure}
        \vspace{-0.5cm}
    \onslide<5->\item This simple approach doesn't work well as is.
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Issues and solutions}\small
\begin{itemize}
    \item \high{Problem}: data in the minibatch is highly correlated
    \begin{itemize}
        \item Consecutive samples are from the same eposide and probably similar states.
        \item Solution: \high{Replay memory}.
        \item You store a large memory buffer of previous $(s,a,r,s')$ (notice this is all you need for Q-learning) and sample from it to get diverse minibatch.
    \end{itemize}
    \item \high{Problem}: The data distribution keeps changing
\begin{itemize}
    \item Since we aren't optimizing $y_i$ its like solving a different (but related) least squares each iteration.
    \item We can stabilize by fixing a \high{target network} for a few iterations 
\end{itemize}
\end{itemize}
    \begin{figure}
    \includegraphics[width=0.75\linewidth]{Figures/DQN}
    \vspace{-0.4cm}
    \caption{Take from:rll.berkeley.edu/deeprlcourse}
\end{figure}
\end{frame}

\begin{frame}\frametitle{Example: DQN on atari}\small
\begin{itemize}
    \item Trained a NN from scratch on atari games 

\begin{figure}
    \includegraphics[width=0.75\linewidth]{Figures/DQN_arc}
\end{figure}
\onslide<2->\item Ablation study
\begin{figure}
    \includegraphics[width=0.75\linewidth]{Figures/ablation}
\end{figure}
\end{itemize}
\end{frame}


\begin{frame}\frametitle{RL recap}\small

\begin{itemize}
    \item Learning from experience not from labeled examples.
    \onslide<2->\item Why is RL hard?
    \begin{itemize}
        \onslide<3->\item Limited feedback.
        \onslide<4->\item Delayed rewards.
        \onslide<5->\item Your model effect what you
         see.
        \onslide<6->\item Huge state space.
    \end{itemize}
    \onslide<7-> \item Usually solved by learning the value function or optimizing the policy (not covered)
    % \onslide<8-> \item Model based method but less successful at the moment.
    \onslide<9-> \item How do you define the rewards? Can be tricky.
    \begin{itemize}
        \item Bad rewards can lead to \high{reward hacking}
        \url{https://www.youtube.com/watch?v=tlOIHko8ySg}
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Q-Learning recap}
\begin{itemize}
    \item Try to find $Q$ that satisfies the optimal Bellman conditions
    \onslide<2->\item \high{Off-policy} algorithm - Doesn't have to follow a greedy policy to evaluate it.
    \onslide<3->\item \high{Model free} algorithm - Doesn't have any model for instantaneous reward or dynamics.
    \onslide<4->\item Learns a seperate value for each $s,a$ pair - doesn't scale up to huge state spaces.
    \onslide<5->\item Can scale using a function approximation
    \begin{itemize}
        \item No more theoretical guarantees.
        \item Can diverge. 
        \item Some simple tricks help a lot.
    \end{itemize}
\end{itemize}
\end{frame}
\end{document}