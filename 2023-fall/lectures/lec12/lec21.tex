\newcommand*{\BeamerSuffix}{-pres}
\newcommand*{\TransSuffix}{-slides}
\documentclass{beamer}
\setbeamertemplate{navigation symbols}{}

\usepackage{movie15}
\usepackage{amssymb,amsmath}
\usepackage{hyperref}
\usepackage{natbib} 
\usepackage{pgf} 
\usepackage{mathtools}
\usepackage{tikz}
\usepackage{bbm}

\usetheme{Madrid}	% clean, nice.  7/12 page numbers


\input{macros}
%\input{defs}
\newcommand{\paramVec}{\mathbf{\theta}}

%%% From defs
\newcommand{\bw}{\mathbf{w}}

%%%
%From Commands.tex

\newcommand{\eqdef}{\triangleq}
\newcommand{\EE}[1]{{\mathbb E}\left[#1\right]}
\newcommand{\EEX}[2]{{\mathbb E}_{#1}\left[#2\right]}
\newcommand{\Qopt}{Q^*}
\newcommand{\Vpi}{V^\pi}
\newcommand{\Qpi}{Q^\pi}
\newcommand{\Vopt}{V^*}
\newcommand{\Qhat}{\hat{Q}}
\newcommand{\Tpi}{T^\pi}
\newcommand{\Topt}{{T^*}}
\newcommand{\piopt}{{\pi^*}}
\newcommand{\argmin}{\mathop{\text{argmin}}}
\newcommand{\argmax}{\mathop{\text{argmax}}}
\newcommand{\States}{\mathcal{S}}
\newcommand{\Actions}{\mathcal{A}}
\newcommand{\PKernel}{\mathcal{P}}
\newcommand{\RKernel}{\mathcal{R}}
\newcommand{\SA}{\States\times\Actions}
\newcommand{\dx}{\mathrm{d}x}
\newcommand{\dy}{\mathrm{d}y}
\newcommand{\dz}{\mathrm{d}z}
\newcommand{\dmu}{\mathrm{d}\mu}
\newcommand{\dnu}{\mathrm{d}\nu}
\newcommand{\drho}{\mathrm{d}\rho}
\newcommand{\ds}{\mathrm{d}s}
\newcommand{\ra}{\rightarrow}
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\renewcommand{\high}{\textbf}

\title[CSC411 2019 Winter Lecture 21]{CSC 411: Introduction to Machine Learning}
\subtitle{CSC 411 Lecture 21: Reinforcement Learning I}
\author[UofT]{Mengye Ren and Matthew MacKay}
\institute[]{University of Toronto}
\date{}


%\beamersetuncovermixins{\opaqueness<1>{25}}{\opaqueness<2->{15}}
\begin{document}


\begin{frame}
  \titlepage
\end{frame}
\setbeamercovered{invisible}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}\frametitle{Reinforcement Learning Problem}\small
\begin{itemize}
	\item In supervised learning, the problem is to predict an output $t$ given an input $x$.
	\item But often the ultimate goal is not to predict, but to make decisions, i.e., take actions.
	\item And we need to take a sequence of actions.
	\item The actions have long-term consequences.
\end{itemize}

\begin{figure}
	\includegraphics[width=0.75\linewidth]{Figures/RL_Problem}
\end{figure}

Reinforcement Learning Problem: An agent continually interacts with the environment. How should it choose its actions so that its long-term rewards are maximized?	
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\begin{frame}\frametitle{Playing Games: Atari}\small
\begin{figure}
\href{run:videos/intro/deep_mind.mp4}{
\includegraphics[width =0.6\linewidth,trim=0 69 0 0,clip]{Figures/robot_game.jpg}\\
{\color{magenta}{\url{https://www.youtube.com/watch?v=V1eYniJ0Rnk}}}
}
\end{figure}
\end{frame}

\begin{frame}\frametitle{Playing Games: Super Mario}\small
\begin{figure}
\href{run:videos/intro/super_mario.mp4}{
\includegraphics[width =0.6\linewidth,trim=0 69 0 0,clip]{Figures/robot_game.jpg}\\
{\color{magenta}{\url{https://www.youtube.com/watch?v=wfL4L_l4U9A}}}
}
\end{figure}
\end{frame}

\begin{frame}\frametitle{Making Pancakes!}\small
\begin{figure}
\href{run:videos/lecture19/pancakes.mp4}{
\includegraphics[width=0.9\linewidth]{Figures/pancakes1}\\
{\color{magenta}{\url{https://www.youtube.com/watch?v=W_gxLKSsSIE}}}
}
\end{figure}
\end{frame}


\begin{frame}\frametitle{Reinforcement Learning Resources}\small
\begin{itemize}

\item {\it Reinforcement Learning: An Introduction second edition}, Sutton \& Barto Book (2018)
\item  \href{https://www.youtube.com/watch?v=2pWv7GOvuf0}{Video lectures by David Silver}
\end{itemize}
\end{frame}

% \begin{frame}\frametitle{What is Reinforcement Learning?}\small
% \begin{figure}
% \includegraphics[width=0.9\linewidth]{../old_course_material/slides/figs/lecture19/rll2}
% \end{figure}
% \vspace{5mm}
% \scriptsize [pic from: Peter Abbeel]
% \end{frame}
% 

\begin{frame}\frametitle{Reinforcement Learning}\small
\begin{itemize}
\item Learning algorithms differ in the information available to learner
\begin{itemize}
\setlength\itemsep{1em}
\onslide<2->\item \high{Supervised}: correct outputs, e.g., class label
\onslide<3->\item \high{Unsupervised}: no feedback, must construct measure of good output
\onslide<4->\item \high{Reinforcement learning}: Reward (or cost)
\end{itemize}
\onslide<5->\item More realistic learning scenario: 
\begin{itemize}
\setlength\itemsep{1em}
\item Continuous stream of input information, and actions\\[0.7mm] 
\onslide<6->\item Effects of action depend on state of the world\\[0.7mm] 
\onslide<7->\item Obtain reward that depends on world state and actions\\[0.7mm] 
\begin{itemize}
\onslide<8->\item You know the reward for your action, not other actions.
\onslide<9->\item Could be a delay between action and reward.
\end{itemize}
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Reinforcement Learning}\small
%\vspace{8mm}

\begin{figure}
\includegraphics[width=0.85\linewidth]{Figures/RL_agent}
\end{figure}

%\begin{figure}
%\includegraphics[width=0.85\linewidth]{Figures/rll3}
%\end{figure}
%\vspace{7mm}
%
%\scriptsize [pic from: Peter Abbeel]
\end{frame}

\begin{frame}\frametitle{Example: Tic Tac Toe, Notation}\small
\vspace{4mm}
\begin{figure}
\includegraphics[width=0.75\linewidth]{Figures/tic1d}
\end{figure}
\vspace{5mm}
\end{frame}

\begin{frame}\frametitle{Example: Tic Tac Toe, Notation}\small
\vspace{4mm}

\begin{figure}
\includegraphics[width=0.75\linewidth]{Figures/tic1c}
\end{figure}
\vspace{5mm}
\end{frame}

\begin{frame}\frametitle{Example: Tic Tac Toe, Notation}\small
\vspace{4mm}

\begin{figure}
\includegraphics[width=0.75\linewidth]{Figures/tic1b}
\end{figure}
\vspace{5mm}
\end{frame}

\begin{frame}\frametitle{Example: Tic Tac Toe, Notation}\small
\vspace{4mm}

\begin{figure}
\includegraphics[width=0.75\linewidth]{Figures/tic1e}
\end{figure}
\vspace{5mm}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}\frametitle{Formalizing Reinforcement Learning Problems}\small
\begin{itemize}
\item Markov Decision Process (MDP) is the mathematical framework to describe RL problems
\item A discounted MDP is defined by a tuple $(\States, \Actions, \PKernel, \RKernel, \gamma)$.
\begin{itemize}
	\item $\States$: State space. Discrete or continuous
	\item $\Actions$: Action space. Here we consider finite action space, i.e., $\Actions = \{a_1, \dotsc, a_{|\Actions|} \}$.
	\item $\PKernel$: Transition probability
	\item $\RKernel$: Immediate reward distribution
	\item $\gamma$: Discount factor ($0 \leq \gamma < 1$)
\end{itemize}
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}\frametitle{Formalizing Reinforcement Learning Problems}\small
\begin{itemize}
\item The agent has a \high{state} $s \in \mathcal{S}$ in the environment, e.g., the location of X and O in tic-tac-toc, or the location of a robot in a room.
\item At every time step $t = 0, 1, \dotsc$, the agent is at state $s_t$.
	\begin{itemize}
	\item Takes an \high{action} $a_t$
	\item Moves into a new state $s_{t+1}$, according to the dynamics of the environment and the selected action, i.e., $s_{t+1} \sim \PKernel(\cdot|s_t, a_t)$
	\item Receives some \high{reward} $r_{t+1} \sim \RKernel(\cdot|s_t, a_t, s_{t+1})$
	\end{itemize}
\end{itemize}
\begin{figure}
\includegraphics[width=0.3\linewidth]{Figures/MDP_transition}
\end{figure}
\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}\frametitle{Formulating Reinforcement Learning}\small

%\begin{figure}
%\includegraphics[width=0.1\linewidth]{Figures/MDP_transition}
%\end{figure}

\begin{itemize}
\item The action selection mechanism is described by a \high{policy} $\pi$
\begin{itemize}
\item Policy $\pi$ is a mapping from states to actions, i.e., $a_t = \pi(s_t)$
\end{itemize}
\item The goal is to find a policy $\pi$ such that \high{long-term rewards} of the agent is maximized. 
\item Different notations of long-term reward:
	\begin{itemize}
	\item
	Average reward:
	\[
	r_{t}+r_{t+1}+r_{t+2}+\dots
	\]
	\item Sometimes a future reward is discounted by $\gamma^{k-1}$, where $k$ is the number of time-steps in the future when it is received:
		\[
	r_{t}+\gamma r_{t+1}+\gamma^2 r_{t+2}+\dots
	\]
		\begin{itemize}
			\item If $\gamma$ close to $1$, rewards further in the future count more, and we say that the agent is ``farsighted''
			\item $\gamma$ is less than $1$ because there is usually a time limit to the sequence of actions needed to solve a task (we prefer rewards sooner rather than 
		\end{itemize}
	\end{itemize}
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}\frametitle{Transition Probability (or Dynamics)}\small
\begin{itemize}
\item The transition probability describes the changes in the state of the agent when it chooses actions
\[
	\PKernel(s_{t+1}=s', r_{t+1}=r' | s_t = s, a_t = a)
\]
%
\item This model has \high{Markov property}: the future depends on the past only through the current state
%
\end{itemize}
\begin{figure}
\includegraphics[width=0.7\linewidth]{Figures/rll6} 
\end{figure}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}\frametitle{Policy}\small
\begin{itemize}
\item A \high{policy} is the action selection mechanism of the agent, and describes its behaviour.
\item Policy can be deterministic or stochastic:
	\begin{itemize}
		\item Deterministic policy: $a = \pi(s)$
		\item Stochastic policy: $A \sim \pi(\cdot|s)$
	\end{itemize}
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}\frametitle{Value Function}\small
\begin{itemize}
\item \high{Value function} is the expected future reward, and is used to evaluate the desirability of states.
\item State-value function $\Vpi$ (or simply value function) for policy $\pi$ is a function defined as
\begin{align*}
	\Vpi(s) \eqdef \EEX{\pi}{\sum_{t \geq 0} \gamma^t R_t \mid S_0 = s}.
\end{align*}
It describes the expected discounted reward if the agent starts from state $s$ and follows policy $\pi$.

\item The action-value function $\Qpi$ for policy $\pi$ is
\begin{align*}
	\Qpi(s,a) \eqdef \EEX{\pi}{\sum_{t \geq 0} \gamma^t R_t \mid S_0 = s, A_0 = a}.
\end{align*}
It describes the expected discounted reward if the agent starts from state $s$, takes action $a$, and afterwards follows policy $\pi$.
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}\frametitle{Value Function}\small

\begin{itemize}
	\item Our aim will be to find a policy $\pi$ that maximizes the value function (the total reward we receive over time): find the policy with the highest expected reward
	\item Optimal value function:
	\[
	  \Qopt(s,a) = \sup_\pi \Qpi(s,a)
	\]

	\item Given $\Qopt$, the optimal policy can be obtained as
	\[
	  \piopt(s) \leftarrow \argmax_a \Qopt(s,a)
	\]

	\item The goal of an RL agent is to find a policy $\pi$ that is close to optimal, i.e., $\Qpi \approx \Qopt$.
\end{itemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}\frametitle{Bellman Equation}\small
The value function satisfies the following recursive relationship:
\begin{align*}
  \Qpi(s,a) & = \EE{\sum_{t=0}^\infty \gamma^t R_t | S_0 = s, A_0 = a }
  \\
  &
  = \EE{R(S_0, A_0) + \gamma \sum_{t = 0}^\infty \gamma^t R_{t+1} | s_0 = s, a_0 = a }
  \\
  &  
  = \EE{R(S_0, A_0) + \gamma \Qpi(S_1, \pi(S_1)) | S_0 = s, A_0 = a }
  \\
  &
  =
  \underbrace{
  r(s,a) + \gamma \int_\States \PKernel(\ds' | s, a) \Qpi (s', \pi(s')) }_{\eqdef (\Tpi \Qpi)(s,a) }
\end{align*}
This is called the Bellman equation and $\Tpi: B(\SA) \ra B(\SA)$ is the Bellman operator.
Similarly, we define the Bellman \emph{optimality} operator:
\begin{align*}
  (\Topt Q)(s,a) \eqdef r(s,a) + \gamma \int_\States \PKernel(\ds' | s,a) \max_{a' \in \Actions} Q(s',a')
\end{align*}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}\frametitle{Bellman Equation}\small
\begin{itemize}
	\item Key observation:
	\begin{align*}
	  & \Qpi = \Tpi \Qpi \\
	  & \Qopt = \Topt \Qopt
	\end{align*}

	\item Value-based approaches try to find a $\Qhat$ such that
	\[
	  \Qhat \approx \Topt \Qhat
	\]

	\item The greedy policy of $\Qhat$ is close to the optimal policy:
	\[
		  % Q^{\pi(x;\Qhat)} \approx Q^{\piopt} = \Qopt
		  Q^{\pi(x;\Qhat)} \approx \Qopt
	\]
	where the greedy policy is defined as
	\[
	  \pi(s; \Qhat) \leftarrow \argmax_{a \in \Actions} \Qhat(s,a)
	\]
\end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}\frametitle{Finding the Optimal Value Function: Value Iteration}\small
\begin{itemize}
	\item Assume that we know the model $\PKernel$ and $\RKernel$. How can we find the optimal value function?
	\item This is the problem of \high{Planning}.
	\item We can benefit from the Bellman optimality equation and use a method called \high{Value Iteration}
		\[
			Q_{k+1} \leftarrow \Topt Q_k
		\]
	\begin{figure}
		\includegraphics[width=0.4\linewidth]{Figures/VI} 
	\end{figure}
\end{itemize}
\vspace{-0.5cm}
\begin{align*}
	&
	Q_{k+1}(s,a) \leftarrow r(s,a) + \gamma \int_{\States} \PKernel(\ds' | s,a) \max_{a' \in \AA} Q_k(s',a')
	\\
	& Q_{k+1}(s,a) \leftarrow r(s,a) + \gamma \sum_{s' \in \States} \PKernel(s' | s,a) \max_{a' \in \AA} Q_k(s',a')
\end{align*}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}\frametitle{Value Iteration}\small
\begin{itemize}
	\item The Value Iteration converges to the optimal value function.
	\item This is because of the contraction property of the Bellman (optimality) operator, i.e., $\norm{\Topt Q_1 - \Topt Q_2}_\infty \leq \gamma \norm{Q_1 - Q_2}_\infty$.
\end{itemize}
	\begin{figure}
		\includegraphics[width=0.4\linewidth]{Figures/VI_Convergence} 
		\vspace{-0.2in}
	\end{figure}
\[
	Q_{k+1} \leftarrow \Topt Q_k
\]
\begin{align*}
	&
	Q_{k+1}(s,a) \leftarrow r(s,a) + \gamma \int_{\States} \PKernel(\ds' | s,a) \max_{a' \in \AA} Q_k(s',a')
	\\
	& Q_{k+1}(s,a) \leftarrow r(s,a) + \gamma \sum_{s' \in \States} \PKernel(s' | s,a) \max_{a' \in \AA} Q_k(s',a')
\end{align*}
\end{frame}

\begin{frame}\frametitle{Maze Example}\small
\begin{figure}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=\linewidth]{Figures/maze1}
\end{minipage}
\hspace{3mm}
\begin{minipage}{0.45\linewidth}
\begin{itemize}
\item Rewards: \onslide<2-> $-1$ per time-step
\item Actions: \onslide<3-> N, E, S, W
\item States: \onslide<4-> Agent's location
\end{itemize}
\end{minipage}
\end{figure}

\vspace{12mm}
\scriptsize [Slide credit: D. Silver]
\end{frame}

\begin{frame}\frametitle{Maze Example}\small
\begin{figure}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=\linewidth]{Figures/maze2}
\end{minipage}
\hspace{3mm}
\begin{minipage}{0.45\linewidth}
\begin{itemize}
\item Arrows represent policy $\pi(s)$ for each state $s$
\end{itemize}
\end{minipage}
\end{figure}

\vspace{14mm}
\scriptsize [Slide credit: D. Silver]
\end{frame}

\begin{frame}\frametitle{Maze Example}\small
\begin{figure}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=\linewidth]{Figures/maze3}
\end{minipage}
\hspace{3mm}
\begin{minipage}{0.45\linewidth}
\begin{itemize}
\item Numbers represent value $V^{\pi}(s)$ of each state $s$
\end{itemize}
\end{minipage}
\end{figure}
\vspace{14mm}
\scriptsize [Slide credit: D. Silver]
\end{frame}

\begin{frame}\frametitle{Example: Tic-Tac-Toe}\small
\begin{itemize}
\item Consider the game tic-tac-toe:
\begin{itemize}
 \setlength\itemsep{1em}
\onslide<2->\item \high{reward}: \onslide<3-> win/lose/tie the game $(+1/-1/0)$ [only at final move in given game]
\onslide<4->\item \high{state}: \onslide<5->positions of X's and O's on the board
\onslide<6->\item \high{policy}: mapping from states to actions 
\begin{itemize}
\onslide<7->\item based on rules of game: choice of one open position
\end{itemize}
\onslide<8->\item \high{value function}: prediction of reward in future, based    on current state
\end{itemize}
\onslide<9->\item In tic-tac-toe, since state space is tractable, we can use a table to represent value function 
\end{itemize}
\end{frame}

\begin{frame}\frametitle{RL \& Tic-Tac-Toe}\small
%\vspace{-0.3cm}
\begin{itemize}
\item Each board position (taking into account symmetry) has some probability
\end{itemize}
\begin{minipage}{4cm}
\begin{center}
\includegraphics[width=1.0\linewidth]{Figures/tic_tac} 
\end{center}
\end{minipage}
\begin{minipage}{7cm}
\begin{itemize}
%\item Each board position (taking into account symmetry) has some probability
\onslide<2->\item Simple learning process: 
\begin{itemize}
\onslide<3->\item start with all values = 0.5
\onslide<4->\item \high{policy}: choose move with highest probability of winning given current legal moves from current state
\onslide<5->\item update entries in table based on outcome of each game
\onslide<6->\item After many games value function will represent true probability of winning from each state
\end{itemize}
%\item Can try alternative policy: sometimes select moves randomly (exploration)
\end{itemize}
\end{minipage}
\begin{itemize}
\onslide<7->\item Can try alternative policy: sometimes select moves randomly (exploration)
\end{itemize}
\end{frame}

\end{document}