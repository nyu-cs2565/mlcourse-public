%% LyX 2.3.4.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english,dvipsnames,aspectratio=169]{beamer}
\usepackage{mathptmx}
\usepackage{eulervm}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{babel}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{ifthen}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{tikz}
\usetikzlibrary{tikzmark}
\usetikzlibrary{calc}
\usepackage{pgfplots}
%\pgfplotsset{compat=1.17}
\usepackage{booktabs}
\usepackage{xpatch}
\usepackage{bbm}

\xpatchcmd{\itemize}
  {\def\makelabel}
  {\ifnum\@itemdepth=1\relax
     \setlength\itemsep{2ex}% separation for first level
   \else
     \ifnum\@itemdepth=2\relax
       \setlength\itemsep{1ex}% separation for second level
     \else
       \ifnum\@itemdepth=3\relax
         \setlength\itemsep{0.5ex}% separation for third level
   \fi\fi\fi\def\makelabel
  }
 {}
 {}

\ifx\hypersetup\undefined
  \AtBeginDocument{%
    \hypersetup{unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=true,
 allcolors=NYUPurple,urlcolor=LightPurple}
  }
\else
  \hypersetup{unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=true,
 allcolors=NYUPurple,urlcolor=LightPurple}
\fi

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
% this default might be overridden by plain title style
\newcommand\makebeamertitle{\frame{\maketitle}}%
% (ERT) argument for the TOC
\AtBeginDocument{%
  \let\origtableofcontents=\tableofcontents
  \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
  \def\gobbletableofcontents#1{\origtableofcontents}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty


% Set Color ==============================
\definecolor{NYUPurple}{RGB}{87,6,140}
\definecolor{LightPurple}{RGB}{165,11,255}


\setbeamercolor{title}{fg=NYUPurple}
\setbeamercolor{frametitle}{fg=NYUPurple}

\setbeamercolor{background canvas}{fg=NYUPurple, bg=white}
\setbeamercolor{background}{fg=black, bg=NYUPurple}

\setbeamercolor{palette primary}{fg=black, bg=gray!30!white}
\setbeamercolor{palette secondary}{fg=black, bg=gray!20!white}
\setbeamercolor{palette tertiary}{fg=gray!20!white, bg=NYUPurple}

\setbeamertemplate{headline}{}
\setbeamerfont{itemize/enumerate body}{}
\setbeamerfont{itemize/enumerate subbody}{size=\normalsize}
\setbeamerfont{itemize/enumerate subsubbody}{size=\normalsize}

\setbeamercolor{parttitle}{fg=NYUPurple}
\setbeamercolor{sectiontitle}{fg=NYUPurple}
\setbeamercolor{sectionname}{fg=NYUPurple}
\setbeamercolor{section page}{fg=NYUPurple}
%\setbeamercolor{description item}{fg=NYUPurple}
%\setbeamercolor{block title}{fg=NYUPurple}

\setbeamertemplate{blocks}[rounded][shadow=false]
\setbeamercolor{block body}{bg=normal text.bg!90!NYUPurple}
\setbeamercolor{block title}{bg=NYUPurple!30, fg=NYUPurple}



\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
\setbeamercolor{section title}{fg=NYUPurple}
 \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\usebeamercolor[fg]{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

\makeatother

\setlength{\parskip}{\medskipamount} 

\input ../macros

\begin{document}
\input ../rosenberg-macros

\title[CSCI-GA 2565]{Support Vector Machine}
\author[]{Mengye Ren
\\ \quad\\
\small (Slides credit to David Rosenberg, He He, et al.)}

\date[]{September 24, 2024}
\institute[]{NYU}

\makebeamertitle
\mode<article>{Just in article version}


% \section{Maximum Margin Classifier}

% \begin{frame}
%     {Linearly Separable Data}
%     Consider a linearly separable dataset $\sD$:
%     \begin{figure}
%         \includegraphics[height=0.5\textheight]{figures/lin-sep-data}
%     \end{figure}
%     Find a separating hyperplane such that
%     \begin{itemize}
%         \item $w^Tx_i > 0$ for all $x_i$ where $y_i=+1$
%         \item $w^Tx_i < 0$ for all $x_i$ where $y_i=-1$
%     \end{itemize}
% \end{frame}

% \begin{frame}
%     {The Perceptron Algorithm}
%     \begin{itemize}
%         \setlength\itemsep{1ex}
%         \item Initialize $w \leftarrow 0$
%         \item While not converged (exists misclassified examples)
%             \begin{itemize}
%                 \item For $(x_i, y_i)\in\sD$
%                     \begin{itemize}
%                         \item If $y_i w^Tx_i < 0$ (wrong prediction)
%                         \item Update $w \leftarrow w + y_ix_i$
%                     \end{itemize}
%             \end{itemize}
%     \end{itemize}
%     \pause
%     \begin{itemize}
%         \item Intuition: move towards misclassified positive examples and away from negative examples
%         \pause
%         \item Guarantees to find a zero-error classifier (if one exists) in finite steps
%         \pause
%         \item What is the loss function if we consider this as a SGD algorithm?
%     \end{itemize}
% \end{frame}

% \section{Minimize the Hinge Loss}
% \begin{frame}
%     {Perceptron Loss}
%     $$
%     \ell(x,y,w) = \max(0, -yw^Tx)
%     $$
%     \begin{figure}
%         \includegraphics[height=0.5\textheight]{figures/perceptron-loss}
%     \end{figure}
% \end{frame}

\begin{frame}{Slides}
\begin{center}
\includegraphics[width=0.3\textwidth]{figures/qr04.png}
\end{center}
\end{frame}

\begin{frame}
    {Maximum-Margin Separating Hyperplane}
    For separable data, there are infinitely many zero-error classifiers.

    Which one do we pick?
    \begin{figure}
        \includegraphics[height=0.5\textheight]{figures/perceptron-hyperplane}
    \end{figure}

    (Perceptron does not return a unique solution.)
\end{frame}

\begin{frame}
    {Maximum-Margin Separating Hyperplane}
    We prefer the classifier that is farthest from both classes of points
    \begin{figure}
        \includegraphics[height=0.5\textheight]{figures/margin}
    \end{figure}
    \begin{itemize}
        \item Geometric margin: smallest distance between the hyperplane and the points 
        \item Maximum margin: \emph{largest} distance to the closest points
    \end{itemize}
\end{frame}

\begin{frame}
    {Geometric Margin}
    We want to maximize the distance between the \hl{separating hyperplane} and the \hl{closest} points.

    Let's formalize the problem.
    \begin{definition}[separating hyperplane]
    We say $(x_i,y_i)$ for $i=1,\ldots,n$ are \textbf{linearly separable} if there
    is a $w\in\BR^d$ and $b\in\BR$ such that $y_i(w^Tx_i+b)>0$ for all
    $i$. The set $\{v\in\BR^d\mid w^Tv+b=0\}$ is called a \textbf{separating hyperplane}.    
  \end{definition}
  \pause

    \begin{definition}[geometric margin]
        Let $H$ be a hyperplane that separates the data $(x_i,y_i)$ for $i = 1,\ldots,n$. The \textbf{geometric margin} of this hyperplane is
        $$
        \min_i d(x_i, H),
        $$
    the distance from the hyperplane to the closest data point.
    \end{definition}
\end{frame}

\begin{frame}
    {Distance between a Point and a Hyperplane}
    \begin{columns}
        \begin{column}{0.5\linewidth}
          \begin{center}
            \includegraphics[width=\linewidth]{"figures/Distance_to_Line"} 
          \end{center}
        \end{column}

        % \begin{column}{0.5\linewidth}
        %     \begin{itemize}
        %         % \item Projection of $v\in\BR^d$ onto $w\in\BR^d$:
        %         %     $\frac{v\cdot w}{\|w\|_2}$
        %         \item Any point on the plane $p$, and normal vector $w/\|w\|_2$
        %         \pause
        %         \item Projection of x onto the normal: $\frac{(x'-p)^T w}{\|w\|_2}$
        %         \pause
        %         \item $(x'-p)^Tw = x'^Tw - p^Tw = x'^Tw + b \ \ (\text{since } p^Tw+b = 0)$
        %         \pause
        %         % \item Distance between $x'$ and $H$:
        %         %     $$
        %         %     d(x', H) = \left |\frac{w^Tx' + b}{\|w\|_2}\right |
        %         %     $$
        %         \item Signed distance between $x'$ and Hyperplane $H$:
        %         $\frac{w^Tx' + b}{\|w\|_2}$
        %         \pause
        %         \item Taking into account of the label $y$:
        %             $d(x', H) = \frac{y(w^Tx' + b)}{\|w\|_2}$
        %     \end{itemize}
        % \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    {Maximize the Margin}
    We want to maximize the geometric margin:
    $$
    \text{maximize} \; \min_{i} d(x_i, H) .
    $$
    \pause
    Given separating hyperplane $H=\pc{v\mid w^Tv + b = 0}$, we have
    $$
    \text{maximize} \; \min_{i} \frac{y_i(w^Tx_i + b)}{\|w\|_2} .
    $$
    \pause
    Let's remove the inner minimization problem by
      $$\begin{array}{ll}
      \text{maximize} & M\\
          \text{subject to} & \frac{y_i(w^Tx_i+b)}{\|w\|_2} \geq M \quad\text{for all
        $i$}
    \end{array}$$
    \pause
    Note that the solution is not unique (why?).
\end{frame}

\begin{frame}
    {Maximize the Margin}
    Let's fix the norm $\|w\|_2$ to $1/M$ to obtain:
      $$\begin{array}{ll}
          \text{maximize} & \frac{1}{\|w\|_2}\\
      \text{subject to} & y_i(w^Tx_i+b) \geq 1 \quad\text{for all
        $i$}
    \end{array}$$
    \pause
    It's equivalent to solving the minimization problem
      $$\begin{array}{ll}
          \text{minimize} & {\frac{1}{2}\|w\|_2^2}\\
      \text{subject to} & y_i(w^Tx_i+b) \geq 1 \quad\text{for all
        $i$}
    \end{array}$$

    Note that $y_i(w^Tx_i+b)$ is the (functional) margin.
    The optimization finds the minimum norm solution which has a margin of at least 1 on all examples.
\end{frame}

\begin{frame}{Not linearly separable}
    What if the data is \textit{not} linearly separable?

    For any $w$, there will be points with a negative margin.
    % \begin{center}
    % \includegraphics[width=0.7\linewidth]{figures/non_separable.pdf}
    % \end{center}
\end{frame}

\begin{frame}{Soft Margin SVM}
    % What if the data is \textit{not} linearly separable?
    % \pause
    % For any $w$, there will be points with a negative margin.
    % \pause
    Introduce \textbf{slack variables} $\xi$'s to penalize small margin:
      $$\begin{array}{ll}
          \text{minimize} & {\frac{1}{2}\|w\|_2^2}
           + \frac{C}{n}\sum_{i=1}^n \xi_i \\
      \text{subject to} & y_i(w^Tx_i+b) \geq 1 - \xi_i \quad\text{for all
        $i$}\\
          & \xi_i \geq 0 \quad\text{for all $i$}
    \end{array}$$
    \begin{itemize}
        \setlength\itemsep{1ex}
        \item If $\xi_i = 0 \;\forall i$, it's reduced to hard SVM.
        \item What does $\xi_i > 0$ mean?
        \item What does $C$ control?
    \end{itemize}
\end{frame}


\begin{frame}
    {Slack Variables}
    $d(x_i, H) = \frac{y_i(w^Tx_i+b)}{\|w\|_2} \ge \frac{1-\xi_i}{\|w\|_2}$,
    thus $\xi_i$ measures the violation by multiples of the geometric margin:
    \begin{itemize}
        \setlength\itemsep{1ex}
        \item $\xi_i=1$: $x_i$ lies on the hyperplane
        \item $\xi_i=3$: $x_i$ is past 2 margin width beyond the decision hyperplane 
    \end{itemize}
    \begin{figure}
        \includegraphics[height=0.6\textheight]{figures/SoftMargin}
    \end{figure}
\end{frame}

\section{Minimize the Hinge Loss}
\begin{frame}
    {Perceptron Loss}
    $$
    \ell(x,y,w) = \max(0, -yw^Tx)
    $$
    \begin{figure}
        \includegraphics[height=0.5\textheight]{figures/perceptron-loss}
    \end{figure}
    If we do ERM with this loss function, what happens?
    %\pdfnote{
    %    Perceptron algorithm. Degenerate solution with w=0.
    %}
\end{frame}

\begin{frame}{Hinge Loss}
\begin{itemize}
\item SVM/Hinge loss: $\loss_{\text{Hinge}}=\max\left\{ 1-m,0\right\} =\left(1-m\right)_{+}$
\item Margin $m=yf(x)$; ``Positive part'' $(x)_{+}=x\ind{x\ge0}$.
\end{itemize}
\begin{center}
\includegraphics[height=0.5\textheight]{figures/loss_Zero_One_Hinge}
\par\end{center}

Hinge is a \textbf{convex}, \textbf{upper bound} on $0-1$ loss. Not
differentiable at $m=1$. \\
We have a \textbf{``margin error''} when $m<1$.
\end{frame}

\begin{frame}{SVM as an Optimization Problem}
\begin{itemize}
\item The SVM optimization problem is equivalent to

\begin{eqnarray*}
    \textrm{minimize} &  & \frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\xi_{i}\\
\textrm{subject to} &  & \xi_{i}\ge\left(1-y_{i}\left[w^{T}x_{i}+b\right]\right)\;\mbox{for }i=1,\ldots,n\\
 &  & \xi_{i}\ge0\;\mbox{for }i=1,\ldots,n
\end{eqnarray*}

\pause
which is equivalent to
\begin{eqnarray*}
    \textrm{minimize} &  & \frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\xi_{i}\\
\textrm{subject to} &  & \xi_{i}\ge\max\left(0,1-y_{i}\left[w^{T}x_{i}+b\right]\right) \;\mbox{for }i=1,\ldots,n.
\end{eqnarray*}
\end{itemize}
\end{frame}


\begin{frame}{SVM as an Optimization Problem}
\begin{eqnarray*}
    \textrm{minimize} &  & \frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\xi_{i}\\
\textrm{subject to} &  & \xi_{i}\ge\max\left(0,1-y_{i}\left[w^{T}x_{i}+b\right]\right) \;\mbox{for }i=1,\ldots,n.
\end{eqnarray*}

\pause
Move the constraint into the objective:
\[
\min_{w\in\reals^{d},b\in\reals}\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\max\left(0,1-y_{i}\left[w^{T}x_{i}+b\right]\right).
\]

\pause
\begin{itemize}
    \item The first term is the L2 regularizer.
    \item The second term is the Hinge loss.
\end{itemize}
\end{frame}

\begin{frame}{Support Vector Machine}
    Using ERM:
\begin{itemize}
\item Hypothesis space $\cf=\left\{ f(x)=w^{T}x+b\mid w\in\reals^{d},b\in\reals\right\} $.
\item $\ell_{2}$ regularization (Tikhonov style)
\item Hinge loss $\ell(m)=\max\left\{ 1-m,0\right\} =\left(1-m\right)_{+}$
\item The SVM prediction function is the solution to
\[
\min_{w\in\reals^{d},b\in\reals}\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\max\left(0,1-y_{i}\left[w^{T}x_{i}+b\right]\right).
\]
% \item Not always differentiable because of the $\max$, but usually ok.
\end{itemize}
\end{frame}

\begin{frame}
    {Summary}
    Two ways to derive the SVM optimization problem:
    \begin{itemize}
    \item Maximize the margin
    \item Minimize the hinge loss with $\ell_2$ regularization
    \end{itemize}
    Both leads to the minimum norm solution satisfying certain margin constraints.

    \begin{itemize}
        \item \textbf{Hard-margin SVM}: all points must be correctly classified with the margin constraints
        \item \textbf{Soft-margin SVM}: allow for margin constraint violation with some penalty
    \end{itemize}
\end{frame}

% \begin{frame}{SVM as an Optimization Problem}
% \[
% \min_{w\in\reals^{d},b\in\reals}\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\max\left(0,1-y_{i}\left[w^{T}x_{i}+b\right]\right).
% \]
% \begin{itemize}
%     \item The first term is the L2 regularizer.
%     \item The second term is the Hinge loss (slack variables).
% \end{itemize}
% \end{frame}

% \begin{frame}{Subgradient Descent}
%     Now that we have the objective, can we do SGD on it?

%     Subgradient: generalize gradient for non-differentiable convex functions
% \end{frame}

\begin{frame}{SVM Optimization Problem}
\begin{itemize}
\item SVM objective function:
\[
    J(w)=\frac{1}{n}\sum_{i=1}^{n}\max\left(0,1-{y_{i}w^{T}x_{i}}\right)+\lambda||w||^{2}.
\]

\pause
\item Not differentiable... but let's think about gradient descent anyway.
\pause
\item Hinge loss: $\ell(m)=\max(0,1-m)$
%\item Derivative of hinge loss $\ell(m)=\max(0,1-m)$:
%\[
%\ell'(m)=\begin{cases}
%0 & m>1\\
%-1 & m<1\\
%\text{undefined} & m=1
%\end{cases}
%\]
\end{itemize}
\begin{eqnarray*}
\del_{w}J(w) & = & \del_{w}\left(\frac{1}{n}\sum_{i=1}^{n}\ell\left(y_{i}w^{T}x_{i}\right)+\lambda||w||^{2}\right)\\
 & = & \frac{1}{n}\sum_{i=1}^{n}\del_{w}\ell\left(y_{i}w^{T}x_{i}\right)+2\lambda w\\
% & = & \begin{cases}
%\frac{1}{n}\sum_{i:y_{i}w^{T}x_{i}<1}\left(-y_{i}x_{i}\right)+2\lambda w & \text{all }y_{i}w^{T}x_{i}\neq1\\
%\text{undefined} & \text{otherwise}
%\end{cases}
\end{eqnarray*}
\end{frame}

\begin{frame}{``Gradient'' of SVM Objective}
    \begin{itemize}
\item Derivative of hinge loss $\ell(m)=\max(0,1-m)$:
$$
\ell'(m)=\begin{cases}
0 & m>1\\
-1 & m<1\\
\text{undefined} & m=1
\end{cases}
$$
\pause

\item By chain rule, we have 
\begin{eqnarray*}
\del_{w}\ell\left(y_{i}w^{T}x_{i}\right) & = & \ell'\left(y_{i}w^{T}x_{i}\right)y_{i}x_{i}\\
& = & \begin{cases}
0 & y_{i}w^{T}x_{i}>1\\
-y_{i}x_{i} & y_{i}w^{T}x_{i}<1\\
\text{undefined} & y_{i}w^{T}x_{i}=1
\end{cases}
\end{eqnarray*}
\end{itemize}
\end{frame}

\begin{frame}{``Gradient'' of SVM Objective}

\begin{eqnarray*}
\del_{w}\ell\left(y_{i}w^{T}x_{i}\right) & = & \begin{cases}
0 & y_{i}w^{T}x_{i}>1\\
-y_{i}x_{i} & y_{i}w^{T}x_{i}<1\\
\text{undefined} & y_{i}w^{T}x_{i}=1
\end{cases}
\end{eqnarray*}

So 
    \vspace{-2ex}
\begin{eqnarray*}
\del_{w}J(w) & = & \del_{w}\left(\frac{1}{n}\sum_{i=1}^{n}\ell\left(y_{i}w^{T}x_{i}\right)+\lambda||w||^{2}\right)\\
 & = & \frac{1}{n}\sum_{i=1}^{n}\del_{w}\ell\left(y_{i}w^{T}x_{i}\right)+2\lambda w\\
 & = & \begin{cases}
\frac{1}{n}\sum_{i:y_{i}w^{T}x_{i}<1}\left(-y_{i}x_{i}\right)+2\lambda w & \text{all }y_{i}w^{T}x_{i}\neq1\\
\text{undefined} & \text{otherwise}
\end{cases}
\end{eqnarray*}
\end{frame}

\begin{frame}{Gradient Descent on SVM Objective?}
\begin{itemize}
\item The gradient of the SVM objective is
\[
\del_{w}J(w)=\frac{1}{n}\sum_{i:y_{i}w^{T}x_{i}<1}\left(-y_{i}x_{i}\right)+2\lambda w
\]
 when $y_{i}w^{T}x_{i}\neq1$ for all $i$, and \hl{otherwise
is undefined}.
\end{itemize}

Potential arguments for why we shouldn't care about the points of
nondifferentiability:\\
\begin{itemize}
\item If we start with a random $w$, will we ever hit exactly $y_{i}w^{T}x_{i}=1$?

\item If we did, could we perturb the step size by $\eps$ to miss such
a point?

\item Does it even make sense to check $y_{i}w^{T}x_{i}=1$ with floating
point numbers?
\end{itemize}
% However, would gradient descent work if the objective is not differentiable?
\end{frame}

\section{Subgradient}
\begin{frame}{First-Order Condition for Convex, Differentiable Function}

\begin{itemize}
\item Suppose $f:\reals^{d}\to\reals$ is \hl{convex} and \hl{differentiable}
Then for any $x,y\in\reals^{d}$
\[
f(y)\ge f(x)+\del f(x)^{T}(y-x)
\]

\item The linear approximation to $f$ at $x$ is a \hl{global underestimator
}of $f$:
\begin{center}
\includegraphics[width=0.5\columnwidth]{figures/BVFig3.2-convexTangent}
\par\end{center}
\item This implies that 
if $\del f(x)=0$ then $x$ is a global minimizer of $f$.
\end{itemize}
\let\thefootnote\relax\footnotetext{\tiny{Figure from Boyd \& Vandenberghe Fig. 3.2; Proof in Section 3.1.3 }}
\end{frame}

\begin{frame}{Subgradients}
\begin{definition}
    A vector $g\in\reals^{d}$ is a \textbf{subgradient} of a \emph{convex} function $f:\reals^{d}\to\reals$
at $x$ if for all $z$, 
$$
f(z)\ge f(x)+g^{T}(z-x).
$$
\end{definition}

\begin{center}
\includegraphics[height=0.4\textheight]{figures/Subderivative_illustration}
\par\end{center}

Blue is a graph of $f(x)$. \\
Each red line $x\mapsto f(x_{0})+g^{T}\left(x-x_{0}\right)$ is a
    \hl{global lower bound} on $f(x)$.
\end{frame}

\begin{frame}
    {Properties}
\begin{definitions}
\begin{itemize}
\item The set of all subgradients at $x$ is called the \textbf{subdifferential}:
$\partial f(x)$ 
\item $f$ is \textbf{subdifferentiable} at $x$ if $\exists$ at least
one subgradient at $x$. 
\end{itemize}
\end{definitions}

For convex functions:\\
    \begin{itemize}
        \setlength\itemsep{1ex}
\item $f$ is differentiable at $x$ iff $\partial f(x)=\left\{ \del f(x)\right\}$.
\item Subdifferential is always non-empty ($\partial f(x)=\emptyset \implies f$ is not convex)
\item $x$ is the global optimum iff $0\in\partial f(x)$.
    \end{itemize}

    For non-convex functions:\\
    \begin{itemize}
        \item The subdifferential may be an empty set (no global underestimator).
    \end{itemize}
\end{frame}

\begin{frame}{Subdifferential of Absolute Value}
\begin{itemize}
\item Consider $f(x)=\left|x\right|$

\end{itemize}
\let\thefootnote\relax\footnotetext{\tiny{Boyd EE364b: Subgradients Slides}}

\begin{center}
\includegraphics[width=0.8\columnwidth]{figures/subgradient-absolute-value}
\par\end{center}
\begin{itemize}
\item Plot on right shows $\left\{ (x,g)\mid x\in\reals,\;g\in\partial f(x)\right\} $ 
\end{itemize}
\end{frame}

% \begin{frame}{Subgradients of $f(x_{1},x_{2})=\left|x_{1}\right|+2\left|x_{2}\right|$}
%     \begin{columns}

%         \begin{column}{0.6\textwidth}
% \begin{itemize}
% \item Let's find the subdifferential of $f(x_{1},x_{2})=\left|x_{1}\right|+2\left|x_{2}\right|$
% at $\left(3,0\right)$.
% \end{itemize}

% \begin{itemize}
% \item First coordinate of subgradient must be $1$, from $\left|x_{1}\right|$
% part (at $x_{1}=3$).
% \end{itemize}

% \begin{itemize}
% \item Second coordinate of subgradient can be anything in $\left[-2,2\right]$.
% \end{itemize}

% \begin{itemize}
% \item So graph of $h(x_{1},x_{2})=f(3,0)+g^{T}\left(x_{1}-3,x_{2}-0\right)$
% is a global underestimate of $f(x_{1},x_{2})$, for any $g=\left(g_{1},g_{2}\right),$
% where $g_{1}=1$ and $g_{2}\in[-2,2]$. 
% \end{itemize}
%     \end{column}
%         \begin{column}{0.3\textwidth}
% \begin{center}
% \includegraphics[height=0.75\textheight]{figures/underestimating-3d-plot-abs-x1-plus-2absx2}\let\thefootnote\relax\footnotetext{\tiny{Plot courtesy of Brett Bernstein.}} 
% \par\end{center}
%         \end{column}
%     \end{columns}
% \end{frame}

% \begin{frame}{Subdifferential on Contour Plot}
% \begin{center}
% \includegraphics[height=0.6\textheight]{figures/subdiff-contour-plot-abs-x1-plus-2absx2}
% \par\end{center}

% \begin{center}
% Contour plot of $f(x_{1},x_{2})=\left|x_{1}\right|+2\left|x_{2}\right|$,
% with set of subgradients at $(3,0)$. .\let\thefootnote\relax\footnotetext{\tiny{Plot courtesy of Brett Bernstein.}}
% \par\end{center}
% \end{frame}

% \begin{frame}
%     {Basic Rules for Calculating Subdifferential}
%     \begin{itemize}
%         \item \head{Non-negative scaling}: $\partial \alpha f(x) = \alpha\partial f(x)$ for $(\alpha > 0)$
%         \pause
%         \item \head{Summation}: $\partial(f_1(x) + f_2(x)) = d_1 + d_2$ for any $d_1\in\partial f_1$ and $d_2 \in\partial f_2$
%         \pause
%         \item \head{Composing with affine functions}: $\partial f(Ax+b) = A^T\partial f(z)$ where $z=Ax+b$
%         \pause
%         \item \head{max}: convex combinations of argmax gradients
%             $$
%             \partial \max(f_1(x), f_2(x)) =
%             \begin{cases}
%                 \nabla f_1(x) & \text{if } f_1(x) > f_2(x), \\
%                 \nabla f_2(x) & \text{if } f_1(x) < f_2(x), \\
%                 \nabla \theta f_1(x) + (1-\theta)f_2(x) & \text{if } f_1(x) = f_2(x), \\
%             \end{cases}
%             $$
%             where $\theta \in[0,1]$.
%     \end{itemize}
% \end{frame}

% \section{Subgradient Descent}
% \begin{frame}{Gradient orthogonal to level sets}
%     We know that gradient points to the fastest ascent direction.
%     What about subgradients?
% \begin{center}
% \includegraphics[height=0.6\textheight]{figures/grad-orthog-to-sublevel-sets}\let\thefootnote\relax\footnotetext{\tiny{Plot courtesy of Brett Bernstein.}}
% \par\end{center}
% \end{frame}

% \begin{frame}{Contour Lines and Subgradients}
% \head{Definition}: A hyperplane $H$ \textbf{supports} a set $S$ if $H$ intersects
% $S$ and all of $S$ lies on one side of $H$.

% \begin{center}
% \includegraphics[width=0.4\textwidth]{figures/Supporting_hyperplane1.png}
% \end{center}
% \end{frame}

% \begin{frame}{Contour Lines and Subgradients}
% \head{Claim}: If $f:\reals^{d}\to\reals$ has subgradient $g$ at $x_{0}$, then
% the hyperplane $H$ orthogonal to $g$ at $x_{0}$ must \textbf{support}
% the level set $S=\left\{ x\in\reals^{d}\mid f(x)=f(x_{0})\right\} $. 

% Proof:\\
% \begin{itemize}
%     \setlength\itemsep{1ex}
% \item For any $y$, we have $f(y)\ge f(x_{0})+g^{T}(y-x_{0})$. (def
% of subgradient)
% \pause
% \item If $y$ is strictly on side of $H$ that $g$ points in,
%     \begin{itemize}
% \item then $g^{T}\left(y-x_{0}\right)>0$.
% \pause
% \item So $f(y)>f(x_{0})$.
% \pause
% \item So $y$ is not in the level set $S$.
% \end{itemize}
% \item $\therefore$ All elements of $S$ must be on $H$ or on the $-g$
% side of $H$.
% \end{itemize}
% \end{frame}

% \begin{frame}{Subgradient of $f(x_{1},x_{2})=\left|x_{1}\right|+2\left|x_{2}\right|$ }
% \begin{center}
% \includegraphics[height=0.6\textheight]{figures/contour-plot-abs-x1-plus-2absx2}\let\thefootnote\relax\footnotetext{\tiny{Plot courtesy of Brett Bernstein.}}
% \par\end{center}
%     \vspace{-1em}
% \begin{itemize}
%     \setlength\itemsep{1ex}
% \item Points on $g$ side of $H$ have larger $f$-values than $f(x_{0})$.
% (from proof)
% \item But points on $-g$ side may \textbf{not }have smaller $f$-values. 
% \item So $-g$ may \textbf{not} be a descent direction. (shown in figure)
% \end{itemize}
% \end{frame}

\begin{frame}
    {Subgradient Descent}
    \begin{itemize}
        \item Move along the negative subgradient:
            $$
            x^{t+1} = x^t - \eta g \quad \text{where } g\in\partial f(x^t) \text{ and } \eta > 0
            $$
        \item This can \al{increase} the objective but gets us \hl{closer to the minimizer} if $f$ is convex and $\eta$ is small enough:
            $$
            \|x^{t+1} - x^*\| < \|x^{t} - x^*\|
            $$
        \item Subgradients don't necessarily converge to zero as we get closer to $x^*$, so we need \al{decreasing step sizes}.
        % , e.g. $O(1/t)$ or $O(1/\sqrt{t})$.
        \item Subgradient methods are \al{slower} than gradient descent.
            % e.g. ${\color{red}O(1/\epsilon^2)}$ vs $O(1/\epsilon)$ for convex functions.
    \end{itemize}
    % \let\thefootnote\relax\footnotetext{\tiny{Based on \url{https://www.cs.ubc.ca/~schmidtm/Courses/5XX-S20/S4.pdf}}}
\end{frame}

\begin{frame}
    {Subgradient descent for SVM}
SVM objective function:
    \vspace{-1em}
\[
    J(w)=\frac{1}{n}\sum_{i=1}^{n}\max\left(0,1-{y_{i}w^{T}x_{i}}\right)+\lambda||w||^{2}.
\]

    Pegasos: stochastic subgradient descent with step size $\eta_t = 1/(t\lambda)$
    \vspace{-1em}
    \begin{figure}
        \includegraphics[height=0.5\textheight]{figures/pegasos}
    \end{figure}
\end{frame}

\begin{frame}
    {Summary}
    \begin{itemize}
        \item Subgradient: generalize gradient for non-differentiable convex functions
        \item Subgradient ``descent'':
            \begin{itemize}
                \item General method for non-smooth functions
                \item Simple to implement
                \item Slow to converge
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{The Dual Problem}
\begin{itemize}
\item In addition to subgradient descent, we can directly solve the optimization problem using a Quadratic Programming (QP) solver.
\item For convex optimization problem, we can also look into its \textbf{dual problem}.
\end{itemize}
\end{frame}

\begin{frame}{SVM as a Quadratic Program}
\begin{itemize}
\item The SVM optimization problem is equivalent to
\begin{eqnarray*}
    \textrm{minimize} &  & \frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\xi_{i}\\
\textrm{subject to} &  & -\xi_{i}\le0\quad\mbox{for }i=1,\ldots,n\\
 &  & \left(1-y_{i}\left[w^{T}x_{i}+b\right]\right)-\xi_{i}\le0\quad\mbox{for }i=1,\ldots,n
\end{eqnarray*}

\item Differentiable objective function
\item $n+d+1$ unknowns and $2n$ affine constraints.
\item A \textbf{quadratic program} that can be solved by any off-the-shelf QP solver. 
\item Let's get more insights by examining the dual.
\end{itemize}
\end{frame}

\begin{frame}{The Lagrangian}

The general {[}inequality-constrained{]} optimization problem is:
\begin{eqnarray*}
\textrm{minimize} &  & f_{0}(x)\\
\textrm{subject to} &  & f_{i}(x)\le0,\;\;i=1,\ldots,m
\end{eqnarray*}

\pause
\begin{definition}
The \textbf{Lagrangian} for this optimization problem is
\[
L(x,\lambda)=f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}f_{i}(x).
\]
\end{definition}
\pause
\begin{itemize}
    \setlength\itemsep{1ex}
    \item $\lambda_{i}$'s are called \textbf{Lagrange multipliers} (also called the \textbf{dual variables}).
    \pause
    \item Weighted sum of the objective and constraint functions
    \pause
    \item Hard constraints $\rightarrow$ soft penalty (objective function)
\end{itemize}
\end{frame}

\begin{frame}
    {Lagrange Dual Function}
    \begin{definition}
        The \textbf{Lagrange dual function} is
        $$
        g(\lambda) = \inf_x L(x, \lambda)
        = \inf_x \p{f_{0}(x)+\sum_{i=1}^{m}\lambda_{i}f_{i}(x)}
        $$
    \end{definition}
    \pause
    \begin{itemize}
        \item $g(\lambda)$ is \hl{concave}
            \note[item]{$g$ is concave because it is the infimum of affine functions. Note that we are not assuming convexity of $f_0$.}
        \pause
        \item \textbf{Lower bound property}: if $\lambda \succeq 0$, $g(\lambda) \le p^*$ where $p^*$ is the optimal value of the optimization problem.
            \note[item]{Note that the proof is straightforward: $\sum_i\lambda_i f_i(x)$ is always negative.}
        \pause
        \item $g(\lambda)$ can be $-\infty$ (uninformative lower bound)
            \note[item]{For example when $L(x,\lambda)$ is affine is $x$.}
    \end{itemize}
    \note[item]{We can consider $g(\lambda)$ as a parametrized lower bound that depends on $\lambda$. 
    So we might want to find $\lambda$ that gives us the best lower bound, which motivates the dual problem.}
\end{frame}

\begin{frame}{The Primal and the Dual}
\begin{itemize}
\item For any \textbf{primal form }optimization problem,
\begin{eqnarray*}
\textrm{minimize} &  & f_{0}(x)\\
\textrm{subject to} &  & f_{i}(x)\le0,\;\;i=1,\ldots,m,
\end{eqnarray*}
there is a recipe for constructing a corresponding \textbf{Lagrangian
dual problem:}
\begin{eqnarray*}
\textrm{maximize} &  & g(\lambda)\\
\textrm{subject to} &  & \lambda_{i}\ge0,\;\;i=1,\ldots,m,
\end{eqnarray*}

% \pause
\item The dual problem is always a convex optimization problem.
% \item The dual variables often have interesting and relevant interpretations.
% \item The dual variables provide certificates for optimality.
\end{itemize}
\end{frame}

\begin{frame}{Weak Duality}
We always have \textbf{weak duality: }$p^{*}\ge d^{*}$.

\begin{center}
\includegraphics[height=0.6\textheight]{figures/weak-duality}
\par\end{center}

\let\thefootnote\relax\footnotetext{\tiny{Plot courtesy of Brett Bernstein.}}
\end{frame}

\begin{frame}{Strong Duality}

For some problems, we have \textbf{strong duality}: $p^{*}=d^{*}$.

\begin{center}
\includegraphics[height=0.6\textheight]{figures/strong-duality}
\par\end{center}

\vspace{-1em}
For convex problems, strong duality is fairly typical. 

\let\thefootnote\relax\footnotetext{\tiny{Plot courtesy of Brett Bernstein.}}
\end{frame}

\begin{frame}{Complementary Slackness}
\begin{itemize}
    \item \hl{Assume strong duality}. Let $x^{*}$ be primal optimal and $\lambda^{*}$ be dual optimal.
Then:
\begin{eqnarray*}
f_{0}(x^{*}) & = & g(\lambda^{*})=\inf_{x}\,L(x,\lambda^{*})\text{\quad(strong duality and definition)}\\
& \le & L(x^{*},\lambda^{*})\\
& = & f_{0}(x^{*})+\sum_{i=1}^{m}{\lambda_{i}^{*}f_{i}(x^{*})}\\
& \le & f_{0}(x^{*}).
\end{eqnarray*}

\end{itemize}
Each term in sum $\sum_{i=1}\lambda_{i}^{*}f_{i}(x^{*})$ must actually
be $0$. That is
\[
    \lambda_i > 0 \implies f_i(x^*) = 0 \quad\text{and}\quad
    f_i(x^*) < 0 \implies \lambda_i = 0 \quad \forall i
\]
This condition is known as \textbf{complementary slackness}. 
\end{frame}

\section{The SVM Dual Problem}
\begin{frame}{SVM Lagrange Multipliers}
    \vspace{-2em}
\begin{eqnarray*}
\textrm{minimize} &  & \frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\xi_{i}\\
\textrm{subject to} &  & -\xi_{i}\le0\quad\mbox{for }i=1,\ldots,n\\
 &  & \left(1-y_{i}\left[w^{T}x_{i}+b\right]\right)-\xi_{i}\le0\quad\mbox{for }i=1,\ldots,n
\end{eqnarray*}

\begin{center}
\begin{tabular}{|c|c|}
\hline 
Lagrange Multiplier & Constraint\tabularnewline
\hline 
\hline 
$\lambda_{i}$ & -$\xi_{i}\le0$\tabularnewline
\hline 
$\alpha_{i}$ & $\left(1-y_{i}\left[w^{T}x_{i}+b\right]\right)-\xi_{i}\le0$\tabularnewline
\hline 
\end{tabular}
\par\end{center}

\[
L(w,b,\xi,\alpha,\lambda)=\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\xi_{i}+\sum_{i=1}^{n}\alpha_{i}\left(1-y_{i}\left[w^{T}x_{i}+b\right]-\xi_{i}\right)+\sum_{i=1}^{n}\lambda_{i}\left(-\xi_{i}\right)
\]

    % \head{Dual optimum value}:
    % $d^* = \sup_{\alpha,\lambda\succeq0}\inf_{w,b,\xi}L(w,b,\xi,\alpha,\lambda)$
    % \note[item]{What are the primal and dual variables?}
\end{frame}

\begin{frame}
    {Strong Duality by Slater's Constraint Qualification}
The SVM optimization problem:
\begin{eqnarray*}
\textrm{minimize} &  & \frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\xi_{i}\\
\textrm{subject to} &  & -\xi_{i}\le0\;\mbox{for }i=1,\ldots,n\\
 &  & \left(1-y_{i}\left[w^{T}x_{i}+b\right]\right)-\xi_{i}\le0\;\mbox{for }i=1,\ldots,n
\end{eqnarray*}

Slater's constraint qualification:\\
    \begin{itemize}
\item Convex problem + affine constraints $\implies$strong duality iff
problem is feasible 
\item Do we have a feasible point?
    \note[item]{Constraints are satisfied by $w=b=0$ and $\xi_{i}=1$ for $i=1,\ldots,n$}
\item For SVM, we have \hl{strong duality}.
    \end{itemize}
\end{frame}

\begin{frame}{SVM Dual Function: First Order Conditions}

Lagrange dual function is the inf over primal variables of $L$: 
\begin{eqnarray*}
 &  & g(\alpha,\lambda)=\inf_{w,b,\xi}L(w,b,\xi,\alpha,\lambda)\\
 & = & \inf_{w,b,\xi}\left[\frac{1}{2}w^{T}w+\sum_{i=1}^{n}\xi_{i}\left(\frac{c}{n}-\alpha_{i}-\lambda_{i}\right)+\sum_{i=1}^{n}\alpha_{i}\left(1-y_{i}\left[w^{T}x_{i}+b\right]\right)\right]
\end{eqnarray*}
    \note[item]{How do we solve the minimization problem?}
    \note[item]{Is it convex? [yes, quadratic term]}

    \vspace{-2em}
\begin{eqnarray*}
\partial_{w}L=0 & \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad & \\
\\
% & \iff & w-\sum_{i=1}^{n}\alpha_{i}y_{i}x_{i}=0\;\iff\;\boxed{w=\sum_{i=1}^{n}\alpha_{i}y_{i}x_{i}}\\
\partial_{b}L=0  & &\\
\\
% & \iff & -\sum_{i=1}^{n}\alpha_{i}y_{i}=0\;\iff\;\boxed{\sum_{i=1}^{n}\alpha_{i}y_{i}=0}\\
\partial_{\xi_{i}}L=0  & &\\
% & \iff & \frac{c}{n}-\alpha_{i}-\lambda_{i}=0\;\iff\;\boxed{\alpha_{i}+\lambda_{i}=\frac{c}{n}}
\end{eqnarray*}
\end{frame}

\begin{frame}{SVM Dual Function}
\begin{itemize}
    \setlength\itemsep{1ex}
\item Substituting these conditions back into $L$, the second term disappears.
\item First and third terms become 
\vspace{0.9in}
% \begin{eqnarray*}
% \frac{1}{2}w^{T}w & = & \frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}\\
% \sum_{i=1}^{n}\alpha_{i}(1-y_{i}\left[w^{T}x_{i}+b\right]) & = & \sum_{i=1}^{n}\alpha_{i}-\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{j}^{T}x_{i}-b\underbrace{\sum_{i=1}^{n}\alpha_{i}y_{i}}_{=0}.
% \end{eqnarray*}

        \vspace{-2ex}
\item Putting it together, the dual function is 
\vspace{0.5in}
% \[
% g(\alpha,\lambda)=\begin{cases}
% \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{j}^{T}x_{i} & \substack{\substack{\sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
% \alpha_{i}+\lambda_{i}=\frac{c}{n}\mbox{, all }i
% }
% }
% \\
% -\infty & \mbox{otherwise.}
% \end{cases}
% \]
\end{itemize}
\end{frame}

\begin{frame}{SVM Dual Problem}
\begin{itemize}
\item The \textbf{dual function} is
\[
g(\alpha,\lambda)=\begin{cases}
\sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{j}^{T}x_{i} & \substack{\substack{\sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
\alpha_{i}+\lambda_{i}=\frac{c}{n}\mbox{, all }i
}
}
\\
-\infty & \mbox{otherwise.}
\end{cases}
\]


\item The \textbf{dual problem }is $\sup_{\alpha,\lambda\succeq0}g(\alpha,\lambda)$:
\begin{eqnarray*}
\sup_{\alpha,\lambda} &  & \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{j}^{T}x_{i}\\
\mbox{s.t.} &  & \sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
 & \quad & \alpha_{i}+\lambda_{i}=\frac{c}{n}\quad\alpha_{i},\lambda_{i}\ge0,\;i=1,\ldots,n
\end{eqnarray*}
 
\end{itemize}
\end{frame}

\section{Insights from the Dual Problem}
\begin{frame}
    {KKT Conditions}
    For \hl{convex} problems, if \hl{Slater's condition} is satisfied, then \textbf{KKT conditions} provide \hl{necessary and sufficient} conditions for the optimal solution.
    \begin{itemize}
        \item Primal feasibility: $f_i(x) \le 0 \quad \forall i$
        \item Dual feasibility: $\lambda \succeq 0$
        \item Complementary slackness: $\lambda_i f_i(x)=0$
        \item First-order condition:
            $$
            \frac{\partial}{\partial x} L(x, \lambda) = 0
            $$
            \note[item]{$x$ needs to be a stationary point of the Lagrangian.}
    \end{itemize}
\end{frame}

\begin{frame}{The SVM Dual Solution}
\begin{itemize}
\item We found the SVM dual problem can be written as:
\begin{eqnarray*}
\sup_{\alpha} &  & \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{j}^{T}x_{i}\\
\mbox{s.t.} &  & \sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
 & \quad & \alpha_{i}\in\left[0,\frac{c}{n}\right]\;i=1,\ldots,n.
\end{eqnarray*}
\end{itemize}

\begin{itemize}
\item Given solution $\alpha^{*}$ to dual, primal solution is $w^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}$. 
\item The solution is in the space spanned by the inputs.
\item Note $\alpha_{i}^{*}\in[0,\frac{c}{n}]$. So $c$ controls max weight
on each example. (\textbf{Robustness}!)
        \begin{itemize}
            \item What's the relation between $c$ and regularization?
                \note[item]{If $c$ is small, the solution is not sensitive to any single example---strong regularization. We can also see this in the primal problem: small $c$ corresponds to larger coefficients for the regularization term.}
        \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Complementary Slackness Conditions}
\begin{itemize}
\item Recall our primal constraints and Lagrange multipliers:
\end{itemize}
\begin{center}
\begin{tabular}{|c|c|}
\hline 
Lagrange Multiplier & Constraint\tabularnewline
\hline 
\hline 
$\lambda_{i}$ & -$\xi_{i}\le0$\tabularnewline
\hline 
$\alpha_{i}$ & $\left(1-y_{i}f(x_{i})\right)-\xi_{i}\le0$\tabularnewline
\hline 
\end{tabular}
\par\end{center}

\begin{itemize}
\item Recall first order condition $\del_{\xi_{i}}L=0$ gave us $\lambda_{i}^{*}=\frac{c}{n}-\alpha_{i}^{*}$.
\item By strong duality, we must have \textbf{complementary slackness}:
\begin{align*}
\alpha_{i}^{*}\left(1-y_{i}f^{*}(x_{i})-\xi_{i}^{*}\right) & =0\\
\lambda_{i}^{*}\xi_{i}^{*}=\left(\frac{c}{n}-\alpha_{i}^{*}\right)\xi_{i}^{*} & =0
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Consequences of Complementary Slackness}
By strong duality, we must have \textbf{complementary slackness}.
\begin{align*}
\alpha_{i}^{*}\left(1-y_{i}f^{*}(x_{i})-\xi_{i}^{*}\right) & =0\\
\left(\frac{c}{n}-\alpha_{i}^{*}\right)\xi_{i}^{*} & =0
\end{align*}

Recall ``\textbf{slack variable}'' $\xi_{i}^{*}=\max\left(0,1-y_{i}f^{*}(x_{i})\right)$
is the hinge loss on $\left(x_{i},y_{i}\right)$.
\begin{itemize}
\item If $y_{i}f^{*}(x_{i})>1$ then the margin loss is $\xi_{i}^{*}=0$,
and we get $\alpha_{i}^{*}=0$.
\item If $y_{i}f^{*}(x_{i})<1$ then the margin loss is $\xi_{i}^{*}>0$,
so $\alpha_{i}^{*}=\frac{c}{n}$. 

\item If $\alpha_{i}^{*}=0$, then $\xi_{i}^{*}=0$, which implies no loss,
so $y_{i}f^{*}(x)\ge1$.

\item If $\alpha_{i}^{*}\in\left(0,\frac{c}{n}\right)$, then $\xi_{i}^{*}=0$,
which implies $1-y_{i}f^{*}(x_{i})=0$.
\end{itemize}
\end{frame}

\begin{frame}{Complementary Slackness Results: Summary}
If $\alpha^{*}$ is a solution to the dual problem, then primal solution
is 
\[
w^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}
    \quad \text{where} \alpha_{i}^{*}\in[0,\frac{c}{n}] .
\]

    Relation between margin and example weights ($\alpha_i$'s):
\begin{eqnarray*}
\alpha_{i}^{*}=0 & \implies & y_{i}f^{*}(x_{i})\ge1\\
\alpha_{i}^{*}\in\left(0,\frac{c}{n}\right) & \implies & y_{i}f^{*}(x_{i})=1\\
\alpha_{i}^{*}=\frac{c}{n} & \implies & y_{i}f^{*}(x_{i})\le1
\end{eqnarray*}
\begin{eqnarray*}
y_{i}f^{*}(x_{i})<1 & \implies & \alpha_{i}^{*}=\frac{c}{n}\\
y_{i}f^{*}(x_{i})=1 & \implies & \alpha_{i}^{*}\in\left[0,\frac{c}{n}\right]\\
y_{i}f^{*}(x_{i})>1 & \implies & \alpha_{i}^{*}=0
\end{eqnarray*}
\end{frame}

\begin{frame}{Support Vectors}
\begin{itemize}
\item If $\alpha^{*}$ is a solution to the dual problem, then primal solution
is 
\[
w^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}
\]
with $\alpha_{i}^{*}\in[0,\frac{c}{n}]$. 

\item The $x_{i}$'s corresponding to $\alpha_{i}^{*}>0$ are called \textbf{support
vectors.}

\item Few margin errors or ``on the margin'' examples $\implies$ \hl{sparsity
in input examples}.
\end{itemize}
\end{frame}


% \section{Kernelization}

\begin{frame}{Dual Problem: Dependence on $x$ through inner products}
\begin{itemize}
\item SVM Dual Problem:
\begin{eqnarray*}
\sup_{\alpha} &  & \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{j}^{T}x_{i}\\
\mbox{s.t.} &  & \sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
 & \quad & \alpha_{i}\in\left[0,\frac{c}{n}\right]\;i=1,\ldots,n.
\end{eqnarray*}


\item Note that all dependence on inputs $x_{i}$ and $x_{j}$ is through
their inner product: $\left\langle x_{j},x_{i}\right\rangle =x_{j}^{T}x_{i}$.

\item We can replace $x_{j}^{T}x_{i}$ by other products... 

\item This is a ``kernelized'' objective function. 
\end{itemize}
\end{frame}


\section{Feature Maps}

\begin{frame}{The Input Space $\cx$}
\begin{itemize}
\item Our general learning theory setup: no assumptions about $\cx$

\item But $\cx=\reals^{d}$ for the specific methods we've developed: 
\begin{itemize}
\item Ridge regression
\item Lasso regression
\item Support Vector Machines 
\end{itemize}
\pause
\item Our hypothesis space for these was all affine functions on $\reals^{d}$:
\[
\cf=\left\{ x\mapsto w^{T}x+b\mid w\in\reals^{d},b\in\reals\right\} .
\]
\pause
\item What if we want to do prediction on inputs not natively in $\reals^{d}$?
\end{itemize}
\end{frame}
%

\begin{frame}{The Input Space $\cx$}
\begin{itemize}
\item Often want to use inputs not natively in $\reals^{d}$:
\begin{itemize}
\item Text documents
\item Image files
\item Sound recordings
\item DNA sequences
\end{itemize}
\pause
\item They may be represented in numbers, but... 
\item The $i$th entry of each sequence should have the same ``meaning''
\item All the sequences should have the same length
\end{itemize}
\end{frame}

\begin{frame}{Feature Extraction}
\begin{definition}
Mapping an input from $\cx$ to a vector in $\reals^{d}$ is called
\textbf{feature extraction} or \textbf{featurization}. 
\end{definition}

\begin{center}
\includegraphics[width=0.9\textwidth]{figures/feature-extraction}
\par\end{center}

\end{frame}

\begin{frame}{Linear Models with Explicit Feature Map}
\begin{itemize}
\item Input space: $\cx$ (no assumptions)
\item Introduce \textbf{feature map} $\phi:\cx\to\reals^{d}$
\item The feature map maps into the \textbf{feature space} $\reals^{d}$.
\pause
\item Hypothesis space of affine functions on feature space:
\[
    \cf=\left\{ x\mapsto w^{T}{\color{blue}\phi(x)}+b\mid w\in\reals^{d},b\in\reals\right\} .
\]
\end{itemize}
\end{frame}
%
\begin{frame}{Geometric Example: Two class problem, nonlinear boundary}
\begin{center}
\includegraphics[height=0.6\textheight]{figures/feature-map-3d}
\par\end{center}
    \vspace{-1em}
\begin{itemize}
\item With identity feature map $\phi(x)=\left(x_{1},x_{2}\right)$ and
linear models, can't separate regions

\item With appropriate featurization $\phi(x)=\left(x_{1},x_{2},x_{1}^{2}+x_{2}^{2}\right)$,
becomes linearly separable . 

\item Video: \url{http://youtu.be/3liCbRZPrZA} 
\end{itemize}
    % \let\thefootnote\relax\footnotetext{\tiny{\url{https://math.stackexchange.com/questions/353607/how-do-inner-product-space-determine-half-planes}}}
\end{frame}
%
\begin{frame}{Expressivity of Hypothesis Space}
\begin{itemize}
\item For linear models, to grow the hypothesis spaces, we must add features.

\item Sometimes we say a larger hypothesis is \hl{more expressive}. 
\begin{itemize}
\item (can fit more relationships between input and action)
\end{itemize}

\item Many ways to create new features.
\end{itemize}
\end{frame}
%

\section{Handling Nonlinearity with Linear Methods}
\begin{frame}{Example Task: Predicting Health}
\begin{itemize}
\item General Philosophy: Extract every feature that might be relevant

\item Features for medical diagnosis
\begin{itemize}
\item height
\item weight
\item body temperature
\item blood pressure
\item etc...
\end{itemize}
\end{itemize}
\let\thefootnote\relax\footnotetext{\tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn 2014. }}
\end{frame}

\begin{frame}{Feature Issues for Linear Predictors}
\begin{itemize}
\item For linear predictors, it's important \textbf{how} features are added
    \begin{itemize}
        \item The relation between a feature and the label may not be linear
        \item There may be complex dependence among features
    \end{itemize}

\pause
\item Three types of nonlinearities can cause problems:

\begin{itemize}
\item Non-monotonicity
\item Saturation
\item Interactions between features
\end{itemize}
\end{itemize}
\let\thefootnote\relax\footnotetext{\tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn 2014. }}
\end{frame}

\begin{frame}{Non-monotonicity: The Issue}
\begin{itemize}
\item Feature Map: $\phi(x)=\left[1,\text{temperature}(x)\right]$

\item Action: Predict health score $y\in\reals$ (positive is good)

\item Hypothesis Space $\cf{=}\left\{ \mbox{affine functions of temperature}\right\} $

\pause
\item Issue: 

\begin{itemize}
\item Health is not an affine function of temperature.

\pause
\item Affine function can either say
\begin{itemize}
\item Very high is bad and very low is good, or
\item Very low is bad and very high is good,
\item But here, both extremes are bad.
\end{itemize}
\end{itemize}
\end{itemize}
\let\thefootnote\relax\footnotetext{\tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn 2014. }}
\end{frame}

\begin{frame}{Non-monotonicity: Solution 1}
\begin{itemize}
\item Transform the input:
\[
\phi(x)=\left[1,\left\{ \text{temperature(x)-37}\right\} ^{2}\right],
\]
where $37$ is ``normal'' temperature in Celsius.
\pause

\item Ok, but requires manually-specified domain knowledge
\begin{itemize}
\item Do we really need that?
\item What does $w^T\phi(x)$ look like?
\end{itemize}
\end{itemize}
\let\thefootnote\relax\footnotetext{\tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn 2014. }}
\end{frame}

\begin{frame}{Non-monotonicity: Solution 2}
\begin{itemize}
\item Think less, put in more:
\[
\phi(x)=\left[1,\text{temperature}(x),\left\{ \text{temperature}(x)\right\} ^{2}\right].
\]


\item \hl{More expressive} than Solution 1.

\end{itemize}
\begin{block}{General Rule}

Features should be simple building blocks that can be pieced together.
\end{block}
\let\thefootnote\relax\footnotetext{\tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn 2014. }}
\end{frame}

\begin{frame}{Saturation: The Issue}
\begin{itemize}
\item Setting: Find products relevant to user's query

\pause
\item Input: Product $x$
\item Output: Score the relevance of $x$ to user's query

\pause
\item Feature Map:
\[
\phi(x)=\left[1,N(x)\right],
\]
where $N(x)=\text{number of people who bought }x$.

\pause
\item We expect a monotonic relationship between $N(x)$ and relevance,
    but also expect \hl{diminishing return}.
\end{itemize}
\let\thefootnote\relax\footnotetext{\tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn 2014. }}
\end{frame}

\begin{frame}{Saturation: Solve with nonlinear transform}
\begin{itemize}
\item Smooth nonlinear transformation:
\[
\phi(x)=\left[1,\log\left\{ 1+N(x)\right\} \right]
\]

\begin{itemize}
    \item $\log\left(\cdot\right)$ good for values with large dynamic ranges
\end{itemize}
\pause
\item Discretization (a discontinuous transformation):
\[
\phi(x)=\left(\ind{0\le N(x)<10},\ind{10\le N(x)<100},\ldots\right)
\]

        \begin{itemize}
\item Small buckets allow quite flexible relationship
\end{itemize}

\end{itemize}

\let\thefootnote\relax\footnotetext{\tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn 2014. }}
\end{frame}


\begin{frame}{Interactions: The Issue}
\begin{itemize}
\item Input: Patient information $x$
\item Action: Health score $y\in\reals$ (higher is better)
\item Feature Map
\[
\phi(x)=\left[\mbox{height}(x),\mbox{weight}(x)\right]
\]

\pause
\item Issue: It's the weight \textit{relative} to the height that's important.

\pause
\item Impossible to get with these features and a linear classifier.
\item Need some \textbf{interaction} between height and weight.

\let\thefootnote\relax\footnotetext{\tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn 2014. }}
\end{itemize}
\end{frame}

\begin{frame}{Interactions: Approach 1}
\begin{itemize}
\item Google ``ideal weight from height''

\item J. D. Robinson's ``ideal weight'' formula:
 % (for a male):
\[
\mbox{weight}\mbox{(kg)}=52+1.9\left[\mbox{height(in)}-60\right]
\]

\pause
\item Make score square deviation between height($h$) and ideal weight($w$)
\[
f(x)=\left(52+1.9\left[h(x)-60\right]-w(x)\right)^{2}
\]

\pause
\item WolframAlpha for complicated Mathematics:
\[
f(x)=3.61h(x)^{2}-3.8h(x)w(x)-235.6h(x)+w(x)^{2}+124w(x)+3844
\]
\end{itemize}
\let\thefootnote\relax\footnotetext{\tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn 2014. }}
\end{frame}

\begin{frame}{Interactions: Approach 2}
\begin{itemize}
\item Just include all second order features:
\[
\phi(x)=\left[1,h(x),w(x),h(x)^{2},w(x)^{2},\underbrace{h(x)w(x)}_{\mbox{cross term}}\right]
\]

\item More flexible, no Google, no WolframAlpha.
\end{itemize}
\begin{block}{General Principle}

Simpler building blocks replace a single ``smart'' feature.
\end{block}
\let\thefootnote\relax\footnotetext{\tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn 2014. }}
\end{frame}

%\begin{frame}{Predicate Features and Interaction Terms}
%\begin{definition}
%A \textbf{predicate} on the input space $\cx$ is a function $P:\cx\to\left\{ \mbox{True},\mbox{False}\right\} $.
%
%\pause{}
%\end{definition}
%
%\begin{itemize}
%\item Many features take this form:
%\begin{itemize}
%\item $x\mapsto s(x)=\ind{\mbox{subject is sleeping}}$
%\item $x\mapsto d(x)=\ind{\mbox{subject is driving}}$
%
%\pause{}
%\end{itemize}
%\item For predicates, interaction terms correspond to \textbf{AND} conjunctions:
%\begin{itemize}
%\item $x\mapsto s(x)d(x)=\ind{\mbox{subject is sleeping AND subject is driving}}$
%\end{itemize}
%\end{itemize}
%\let\thefootnote\relax\footnotetext{\tiny{From Percy Liang's "Lecture 3" slides from Stanford's CS221, Autumn 2014. }}
%\end{frame}

\begin{frame}{Monomial Interaction Terms}
    \textbf{Interaction terms} are useful building blocks to model non-linearities in features.
\begin{itemize}
\item Suppose we start with $x=\left(1,x_{1},\ldots,x_{d}\right)\in\reals^{d+1}=\cx$.

\pause
\item Consider adding all \textbf{monomials} of degree $M$: $x_{1}^{p_{1}}\cdots x_{d}^{p_{d}}$,
with $p_{1}+\cdots+p_{d}=M$.

        \begin{itemize}
            \item Monomials with degree 2 in 2D space: $x_1^2$, $x_2^2$, $x_1x_2$
        \end{itemize}

% \pause
% \item How many features will we end up with? 
% \pause
% \begin{center}
% \includegraphics[width=0.2\textwidth]{figures/star-and-bar.png}
% \end{center}
% \pause
% ${M+d-1 \choose M}$
% (``stars and bars'')
    % \pdfnote{Distribute M degrees/objects to d boxes. Place (n-1) bars among M indistinguishable objects.}
% \item This leads to extremely \al{large data matrices}
% \begin{itemize}
% \item For $d=40$ and $M=8$, we get $314457495$ features.
% \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Big Feature Spaces}
This leads to extremely \al{large data matrices}
\begin{itemize}
\item For $d=40$ and $M=8$, we get $314457495$ features.
\end{itemize}

\pause
Very large feature spaces have two potential issues:\\
\begin{itemize}
\item Overfitting
\item Memory and computational costs 
\end{itemize}

\pause
    Solutions:\\
\begin{itemize}
\item Overfitting we handle with regularization.
\item \textbf{Kernel methods} can help with memory and
    computational costs when we go to high (or infinite) dimensional spaces.
\end{itemize}
\end{frame}

\end{document}
