%% LyX 2.3.4.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english,dvipsnames,aspectratio=169]{beamer}
\usepackage{mathptmx}
\usepackage{algorithm2e}
\usepackage{eulervm}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{babel}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{ifthen}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{tikz}
\usetikzlibrary{tikzmark}
\usetikzlibrary{calc}
\usepackage{pgfplots}
%\pgfplotsset{compat=1.17}
\usepackage{booktabs}
\usepackage{xpatch}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{pgfpages}
\usepackage{bbm}


\xpatchcmd{\itemize}
  {\def\makelabel}
  {\ifnum\@itemdepth=1\relax
     \setlength\itemsep{2ex}% separation for first level
   \else
     \ifnum\@itemdepth=2\relax
       \setlength\itemsep{1ex}% separation for second level
     \else
       \ifnum\@itemdepth=3\relax
         \setlength\itemsep{0.5ex}% separation for third level
   \fi\fi\fi\def\makelabel
  }
 {}
 {}

\ifx\hypersetup\undefined
  \AtBeginDocument{%
    \hypersetup{unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=true,
 allcolors=NYUPurple,urlcolor=LightPurple}
  }
\else
  \hypersetup{unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},pdfborderstyle={},backref=false,colorlinks=true,
 allcolors=NYUPurple,urlcolor=LightPurple}
\fi

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
% this default might be overridden by plain title style
\newcommand\makebeamertitle{\frame{\maketitle}}%
% (ERT) argument for the TOC
\AtBeginDocument{%
  \let\origtableofcontents=\tableofcontents
  \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
  \def\gobbletableofcontents#1{\origtableofcontents}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty


% Set Color ==============================
\definecolor{NYUPurple}{RGB}{87,6,140}
\definecolor{LightPurple}{RGB}{165,11,255}


\setbeamercolor{title}{fg=NYUPurple}
\setbeamercolor{frametitle}{fg=NYUPurple}

\setbeamercolor{background canvas}{fg=NYUPurple, bg=white}
\setbeamercolor{background}{fg=black, bg=NYUPurple}

\setbeamercolor{palette primary}{fg=black, bg=gray!30!white}
\setbeamercolor{palette secondary}{fg=black, bg=gray!20!white}
\setbeamercolor{palette tertiary}{fg=gray!20!white, bg=NYUPurple}

\setbeamertemplate{headline}{}
\setbeamerfont{itemize/enumerate body}{}
\setbeamerfont{itemize/enumerate subbody}{size=\normalsize}

\setbeamercolor{parttitle}{fg=NYUPurple}
\setbeamercolor{sectiontitle}{fg=NYUPurple}
\setbeamercolor{sectionname}{fg=NYUPurple}
\setbeamercolor{section page}{fg=NYUPurple}
%\setbeamercolor{description item}{fg=NYUPurple}
%\setbeamercolor{block title}{fg=NYUPurple}

\setbeamertemplate{blocks}[rounded][shadow=false]
\setbeamercolor{block body}{bg=normal text.bg!90!NYUPurple}
\setbeamercolor{block title}{bg=NYUPurple!30, fg=NYUPurple}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
\setbeamercolor{section title}{fg=NYUPurple}
 \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\usebeamercolor[fg]{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

\makeatother

\setlength{\parskip}{\medskipamount} 

\input ../macros

\begin{document}
\input ../rosenberg-macros

%\setbeameroption{show notes on second screen}

\title[CSCI-GA 2565]{Structured Prediction \& Decision Trees}
\author[]{Mengye Ren
\\ \quad\\
\small (Slides credit to David Rosenberg, He He, et al.)}
% \author{He He \\
% Slides based on Lecture
% \href{https://github.com/davidrosenberg/mlcourse/blob/gh-pages/Lectures/09.multiclass.pdf}{09} from David Rosenberg's course materials (\url{https://github.com/davidrosenberg/mlcourse})
% }
\date[]{Nov 5, 2024}
\institute[]{NYU}

\makebeamertitle
\mode<article>{Just in article version}


%================================== 

\begin{frame}{Slides}
\begin{center}
\includegraphics[width=0.35\textwidth]{figures/qr_8.png}
\end{center}
\end{frame}

\begin{frame}{Final Project}
\begin{itemize}
  \item Project Consultation is mandatory. I am meeting with all 22 teams. 
  \item Every member in the group needs to be present.
  \item If you have an emergency situation and cannot attend, you need to let me know. 
  \item You may lose participation marks for not showing up in the consultation session without a reason.
  \item If you plan to show up, be on time. Your team member's time is also valuable.
\end{itemize}
\end{frame}

\begin{frame}{Midterm}
\begin{itemize}
  \item Out of 30: Average: 16.05, Std: 4.29
  \item 9 free marks, capped at 30.
  \item Regrade request --- email graders.
\end{itemize}
\begin{center}
\includegraphics[width=0.6\textwidth]{figures/midterm.png}
\end{center}
\end{frame}

\section{Scientific Writing}

\begin{frame}{Introduction}
\begin{itemize}
  \item Introduction is the road map of the full report. Here is a general structure.
  \item<+-> Paragraph 1
  \begin{itemize}
    \small
  \item<1-> What are some broad context of the problem? $\checkmark$
  \item<1-> What is the problem that you are studying? $\checkmark$
  \item<1-> Why is the problem interesting? $\checkmark$
  \end{itemize}
  \item<+-> Paragraph 2
  \begin{itemize}
    \small
    \item<2-> Historically, what have people been doing in the space of similar problems?
    \item<2-> What is the gap and what extra can this project bring?
  \end{itemize}
  \item<+-> Paragraph 3
  \begin{itemize}
    \small
    \item<3-> What technical approach are you taking?
    \item<3-> What dataset have you experimented with and what are the core results?
    \item<3-> What are some broader impact of the work?
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Example}

{\small
{\bf Introduction}

Cybersecurity threats have become a major concern, posing risks to both personal data and organizational assets. In 2023, cyberattacks led to financial damages of over \$10 billion in the United States alone, according to recent reports. Globally, these numbers were even more concerning, with the cost of cybercrime expected to surpass \$8 trillion in 2023, as noted by cybersecurity researchers. In the U.S., approximately 66\% of organizations experienced at least one form of cyberattack in 2023. These figures highlight an urgent need for robust strategies to detect, prevent, and mitigate cybersecurity breaches across industries.
}
\pause

\textcolor{red}{What are the issues?}

\end{frame}


\begin{frame}{Revised Example}
\small
\textcolor{green}{The rapid escalation in cybersecurity threats has led to significant financial and operational impacts, with damages in the United States alone exceeding \$10 billion in 2023, according to the Cybersecurity \& Infrastructure Security Agency (CISA) [1].} 
\textcolor{orange}{Despite advancements in cybersecurity solutions, traditional rule-based systems often struggle to detect new or sophisticated attack patterns, especially as cybercriminals evolve their techniques to evade static detection rules. This limitation highlights a critical gap: the need for adaptable, data-driven methods that can dynamically learn and respond to emerging threats.}

Continue...
\end{frame}

\begin{frame}{Revised Example}
\small
\textcolor{blue}{In response to this gap, our project applies machine learning to develop a robust intrusion detection system that leverages a hybrid approach combining supervised and unsupervised learning.} 
\textcolor{cyan}{Using a labeled dataset of network traffic, our model is first trained to distinguish normal from suspicious activity. To enhance adaptability, we also employ an anomaly detection module using autoencoders, which identifies deviations from typical patterns, even for attack types not present in the training data.}

\textcolor{purple}{Our results demonstrate the effectiveness of our approach. The model achieved an accuracy of 94\% in detecting known attack types and reduced false positives by 30\% compared to a traditional rule-based system. Furthermore, the anomaly detection module identified previously unseen attack patterns with an 87\% true positive rate.} 
\textcolor{pink}{These results suggest that our machine learning-based system can offer a more flexible and accurate defense mechanism against evolving cybersecurity threats.}
\end{frame}

\begin{frame}{Abstract}
Abstract is the summary of the introduction. Make it into one paragraph.

{\small
\textcolor{green}{Cybersecurity threats are increasing in frequency and sophistication, resulting in substantial financial losses and operational disruptions.}
\textcolor{orange}{Traditional rule-based detection systems are limited in their ability to identify novel attack patterns, creating a need for adaptable, data-driven solutions.}
\textcolor{blue}{This project presents a machine learning-based intrusion detection system designed to address this gap by combining supervised learning for known threats with an unsupervised anomaly detection module to identify emerging attack types.}
\textcolor{purple}{Experimental results demonstrate the model's effectiveness, achieving 94\% accuracy in detecting known attacks and an 87\% true positive rate for novel threats, while reducing false positives by 30\% compared to rule-based methods.}
\textcolor{pink}{These findings suggest that our hybrid approach provides a more flexible and accurate defense against evolving cyber threats.}
}
\end{frame}

\begin{frame}{Related Work}
\begin{itemize}
  \item Survey historical attempts of similar problems (not necessarily the same problem!)
  \item What categories do historical approaches span across? Which category does your method fall into?
  \item What is the relation of your work in the context of prior literature?
\end{itemize}
\end{frame}

\begin{frame}{Example}
\small
Intrusion detection has been approached from several angles, with methods broadly categorized into rule-based systems, supervised learning-based intrusion detection, unsupervised anomaly detection, and hybrid models.

\textbf{Rule-Based Detection:}
Traditional rule-based systems, such as Snort, use known attack signatures to detect intrusions [1]. While effective for known threats, these systems fail to recognize novel or evolving attacks [2]. Our approach seeks to address this limitation by incorporating machine learning for greater adaptability.

\textbf{Supervised Machine Learning:}
Supervised models like support vector machines and neural networks are commonly used due to their high accuracy with labeled data, effectively identifying known attacks [3, 4]. However, their reliance on labeled datasets makes them less adaptable to new threats. Our project builds on supervised methods but adds an anomaly detection layer to handle unknown attacks.

Continue...
\end{frame}

\begin{frame}{Example}
\small
\textbf{Unsupervised and Anomaly-Based Detection:}
Unsupervised methods, including autoencoders and clustering, detect unusual patterns in network traffic without labeled data [6, 7]. While flexible, these techniques often produce higher false-positive rates, posing practical challenges [8]. Our model incorporates an autoencoder for anomaly detection, with optimized thresholds to reduce false positives.

\textbf{Hybrid Approaches:}
Hybrid models combine supervised and unsupervised techniques to balance accuracy and adaptability. For example, hybrid models like those by Huang et al. (2021) integrate anomaly detection within supervised frameworks, achieving lower false-positive rates [9]. Our work further advances these methods by embedding an autoencoder anomaly module into a supervised model with optimized thresholds, aiming for reliable real-time intrusion detection.
\end{frame}

\begin{frame}{Figures and Tables}
\begin{itemize}
  \item<+-> When there is a strong trend, use a figure.
  \item<+-> Try to export as PDF instead of JPG/PNG (rasterized).
  \item<+-> Use bigger font size in the figure (same or slightly smaller than the main text).
\end{itemize}
\begin{center}
\includegraphics[width=0.6\textwidth]{figures/plot_example.png}
\end{center}
\end{frame}

\begin{frame}{Figures and Tables}
\begin{itemize}
  \item<+-> When you need to emphasize a small difference, use a table.
  \item<+-> When the table is too wide, use \texttt{resizebox} to fit your table with the text width.
  \item<+-> Keep the same number of significant digits.
  \item<+-> Explain the Figure and Table in the caption.
  \item<+-> Nice to have: Bold the column/row and best numbers. Highlight the important rows.
\end{itemize}
\begin{center}
\includegraphics[width=0.6\textwidth]{figures/table_example.png}
\end{center}
\end{frame}

\section{Introduction to Structured Prediction}
\begin{frame}{Example: Part-of-speech (POS) Tagging}

\begin{itemize}
\item Given a sentence, give a part of speech tag for each word:
\end{itemize}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline 
$x$ & $\underbrace{\mbox{[START]}}_{x_{0}}$ & $\underbrace{\mbox{He}}_{x_{1}}$ & $\underbrace{\mbox{eats}}_{x_{2}}$ & $\underbrace{\mbox{apples}}_{x_{3}}$\tabularnewline
\hline 
$y$ & $\underbrace{\mbox{[START]}}_{y_{0}}$ & $\underbrace{\mbox{Pronoun}}_{y_{1}}$ & $\underbrace{\mbox{Verb}}_{y_{2}}$ & $\underbrace{\mbox{Noun}}_{y_{3}}$\tabularnewline
\hline 
\end{tabular}
\par\end{center}

\pause{}
\mode<handout>{
\begin{itemize}
\item $\cv=\left\{ \mbox{all English words}\right\} \cup\left\{ \mbox{[START]},"."\right\} $
\item $\cx=\cv^{n}$, $n=1,2,3,\ldots$ {[}Word sequences of any length{]}

\pause{}
\item $\cp=\left\{ \mbox{START},\mbox{Pronoun,Verb,Noun,Adjective}\right\} $
\item $\cy=\cp^{n},\,n=1,2,3,\ldots${[}Part of speech sequence of any length{]}
\end{itemize}
}
\mode<presentation>{
  \vspace{1.0in}
}
\end{frame}

\begin{frame}{Example: Action grounding from long-form videos}
\begin{itemize}
  \item Given a long video, segment the video into short windows where each window corresponds to an action from a list of actions.
  \item E.g. slicing, chopping, frying, washing, etc.

  \mode<handout>{
  \item $\cv=\mathbb{R}^D $ image features
  \item $\cx=\cv^{n}$, $n=1,2,3,\ldots$ {[}video frame length{]}
  \item $\cp=\left\{ \mbox{Slicing,Chopping,Frying,...}\right\} $
  \item $\cy=\cp^{n},\,n=1,2,3,\ldots${[}Part of speech sequence of any length{]}
  \item Can also be represented with start and end timestamps.
  }
  \mode<presentation>{
  \vspace{1.3in}
  }
\end{itemize}
\end{frame}

\begin{frame}{Multiclass Hypothesis Space}

\begin{itemize}
\item \textcolor{blue}{Discrete} output space: $\cy(x)$ 
\begin{itemize}
\item Very large but has structure, e.g., linear chain (sequence labeling), tree (parsing)
\item Size depends on input $x$
\end{itemize}

\pause{}
\item Base Hypothesis Space: $\ch=\left\{ h:\cx\times\cy\to\reals\right\} $
\begin{itemize}
\item $h(x,y)$ gives \textcolor{blue}{compatibility score} between input $x$ and
output $y$ 
\end{itemize}

\pause{}
\item Multiclass hypothesis space
\[
\cf=\left\{ x\mapsto\argmax_{y\in\cy}h(x,y)\mid h\in\ch\right\} 
\]

\item Final prediction function is an $f\in\cf$. 
\item For each $f\in\cf$ there is an underlying compatibility score function
$h\in\ch$. 
\end{itemize}
\end{frame}
%

\begin{frame}{Structured Prediction}
\begin{itemize}
\item Part-of-speech tagging
\begin{table}
\begin{tabular}{llll}
$x\colon$ & he & eats & apples \\
$y\colon$ & pronoun & verb & noun 
\end{tabular}
\end{table}

\item Multiclass hypothesis space:
\begin{align}
& h(x,y) = w^T \Psi(x, y) \\
& \cf = \left\{ x\mapsto\argmax_{y\in\cy}h(x,y)\mid h\in\ch\right\}
\end{align}
\end{itemize}

\begin{itemize}
\item A special case of multiclass classification
\item  How to design the feature map $\Psi$? What are the considerations?
\note{contextual dependence, efficient argmax}
\end{itemize}
\end{frame}
%
\begin{frame}{Unary features}

\begin{itemize}
\item A  \textbf{unary feature} only depends on 
\begin{itemize}
\item the label at a \textcolor{blue}{single position}, $y_{i}$, and $x$ 
\end{itemize}

\item Example: 
\begin{eqnarray*}
\phi_{1}(x,y_{i}) & = & \ind{x_{i}=\mbox{runs}}\ind{y_{i}=\mbox{Verb}}\\
\phi_{2}(x,y_{i}) & = & \ind{x_{i}=\mbox{runs}}\ind{y_{i}=\mbox{Noun}}\\
\phi_{3}(x,y_{i}) & = & \ind{x_{i-1}=\mbox{He}}\ind{x_{i}=\mbox{runs}}\ind{y_{i}=\mbox{Verb}}
\end{eqnarray*}
 
\end{itemize}
\end{frame}
%
\begin{frame}{Markov features}

\begin{itemize}
\item A \textbf{markov feature} only depends on 
\begin{itemize}
\item two \textcolor{blue}{adjacent} labels, $y_{i-1}$ and $y_{i}$,
and $x$
\end{itemize}

\item Example: 
\begin{eqnarray*}
\theta_{1}(x,y_{i-1},y_{i}) & = & \ind{y_{i-1}=\mbox{Pronoun}}\ind{y_{i}=\mbox{Verb}}\\
\theta_{2}(x,y_{i-1},y_{i}) & = & \ind{y_{i-1}=\mbox{Pronoun}}\ind{y_{i}=\mbox{Noun}}
\end{eqnarray*}

\item Reminiscent of Markov models in the output space
\item Possible to have higher-order features 
 
\end{itemize}
\end{frame}
%
\begin{frame}{Local Feature Vector and Compatibility Score}
\begin{itemize}[<+->]
\item At each position $i$ in sequence, define the \textbf{local feature
vector} (\textcolor{blue}{unary} and \textcolor{red}{markov}):
\begin{eqnarray*}
\Psi_{i}(x,y_{i-1},y_{i}) & = & (\phi_{1}(x,{\color{blue}y_{i}}),\phi_{2}(x,{\color{blue}y_{i}}),\ldots,\\
 &  & \theta_{1}(x,{\color{red}y_{i-1},y_{i}}),\theta_{2}(x,{\color{red}y_{i-1},y_{i}}),\ldots)
\end{eqnarray*}

\item And \textbf{local compatibility score} at position $i$:
$\left\langle w,\Psi_{i}(x,y_{i-1},y_{i})\right\rangle $. 

\item The compatibility score for $\left(x,y\right)$
is the sum of local compatibility scores: 
\begin{align}
\sum_{i}\left\langle w,\Psi_{i}(x,y_{i-1},y_{i})\right\rangle 
= \left\langle w,\sum_{i}\Psi_{i}(x,y_{i-1},y_{i})\right\rangle
= \left\langle w,\Psi(x,y)\right\rangle ,
\end{align}
where we define the \textbf{sequence feature vector} by 
\[
\Psi(x,y)=\sum_{i}\Psi_{i}(x,y_{i-1},y_{i}). \qquad \text{\color{blue}decomposable}
\]
\end{itemize}
\end{frame}

\begin{frame}
{Structured perceptron}
\begin{algorithm}[H]
  Given a dataset $\sD=\pc{(x, y)}$\;
  Initialize $w\leftarrow 0$\;
  \For{$\text{iter} = 1,2,\ldots,T$}
  {
  \For{$(x,y) \in \sD$}
  {
    $\hat{y} = \argmax_{y'\in{\color<2->{red}\sY(x)}} w^T\psi(x, y')$\;
    \If(\tcp*[h]{We've made a mistake}){$\hat{y} \neq y$}{ 
    $w \leftarrow w + \Psi(x, y)$ \tcp*[l]{Move the scorer towards $\psi(x, y)$}
    $w \leftarrow w - \Psi(x, \hat{y})$ \tcp*[l]{Move the scorer away from $\psi(x, \hat{y})$}
    }
    }
    }
\end{algorithm}
\onslide<2->{
Identical to the multiclass perceptron algorithm except the $\argmax$ is now over the structured output space $\sY(x)$.
}
\end{frame}
%

\begin{frame}
{Structured hinge loss}
\begin{itemize}
\item Recall the generalized hinge loss
\begin{align}
\ell_\text{hinge}(y, \hat{y}) \eqdef
\max_{y'\in{\color{blue}\sY(x)}}\p{
\Delta(y, {y'}) +
\left\langle w, \p{\Psi(x, {y'}) - \Psi(x, {y}}) \right\rangle
}
\end{align}

\item What is $\Delta(y, {y'})$ for two sequences?
\pause
\item \textbf{Hamming loss} is common:
\[
\Delta(y,y')=\frac{1}{L}\sum_{i=1}^{L}\ind{y_{i}\neq y_{i}'}
\]
where $L$ is the sequence length.

%\item Can generalize to the cost-sensitive version using $\delta(y_{i},y_{i}')$

\end{itemize}
\end{frame}

\begin{frame}
{Structured SVM}
\textcolor{Green}{Exercise}:
\begin{itemize}
\item Write down the objective of structured SVM using the structured hinge loss.

\item Stochastic sub-gradient descent for structured SVM (similar to HW3 P3)
\item Compare with the structured perceptron algorithm
\end{itemize}
\end{frame}

\begin{frame}
{The argmax problem for sequences}
% \pause
\begin{description}[<+->]
\item[Problem]
To compute predictions, we need to find 
$\argmax_{y\in\cy(x)}\left\langle w,\Psi(x,y)\right\rangle$, and $\left|\cy(x)\right|$ is \textcolor{blue}{exponentially} large.

\item[Observation]
$\Psi(x,y)$ decomposes to $\sum_i \Psi_i(x,y)$.

\item[Solution]
Dynamic programming
\includegraphics[height=0.4\textheight]{figures/dp}
What's the running time?
\note<.>{Let $K=\vert\sY\vert$, DP runtime $O(K^2L)$, $m$th order Markov feature has runtime $O(K^mL)$, naive runtime $O(K^L)$.}
\end{description}
\let\thefootnote\relax\footnotetext{\tiny{Figure by Daum\'e III. A course in machine learning. Figure 17.1}.}
\end{frame}

\begin{frame}{Dynamic Programming (MAP inference)}
\begin{center}
\includegraphics[height=0.4\textheight]{figures/dp}
\end{center}
\vspace{-0.1in}
\begin{itemize}
  \item Initiate $\alpha_j(1) = \exp (w^\top \psi(y_1=j, x_1))$
  \pause
  \item Recursion: $\alpha_j(t) = \max_i \alpha_i (t-1)+ w^\top \psi(y_t=j, y_{t-1}=i, x_t)$
  \pause
  \item $p_j(t) = \argmax ...$ (why?)
  % \item Result: $Z(x) = \sum_j \alpha_j(T)$
  \item Problem-specific: DP, graph cuts, etc. General: integer linear programming (ILP).
\end{itemize}
\end{frame}

% \begin{frame}
% {The argmax problem in general}
% Efficient problem-specific algorithms:
% \begin{table}
% \begin{tabular}{llll}
% \toprule
% problem & structure & algorithm \\
% \midrule
% constituent parsing & binary trees with context-free features & CYK  \\
% dependency parsing & spanning trees with edge features & Chu-Liu-Edmonds \\
% image segmentation & 2d with adjacent-pixel features & graph cuts \\
% \bottomrule
% \end{tabular}
% \end{table}
% \pause

% General algorithm:
% \begin{itemize}
% \item Integer linear programming (ILP)
% \begin{align}
% \max_z\; a^Tz \quad \text{s.t. linear constraints on $z$}
% \end{align}
% \item $z$: indicator of substructures, \eg $\1\pc{y_i=\text{article and }y_{i+1}=\text{noun}}$
% \item constraints: $z$ must correspond to a valid structure
% \end{itemize}
% \end{frame}

% Additional resource:
% https://people.cs.umass.edu/~mccallum/papers/crf-tutorial.pdf
\begin{frame}{Conditional random field (CRF)}
\begin{itemize}
\item Recall that we can write logistic regression in a general form:
\[
p(y|x) = \frac{1}{Z(x)} \exp(w^\top \psi(x, y)).
\]
\item $Z$ is normalization constant: $Z(x) = \sum_{y \in Y} \exp(w^\top \psi(x, y))$.
\pause
\item Example: linear chain $\{y_t\}$
\item We can incorporate unary and Markov features:
$p(y|x) = \frac{1}{Z(x)} \exp(\sum_t w^\top \psi(x, y_t, y_{t-1}))$
\begin{center}
\includegraphics[width=0.6\textwidth]{figures/crf.png}
\end{center}
% \item Unary energy
% \item Pairwise energy
% \item Total energy
\end{itemize}
\end{frame}

\begin{frame}{Conditional random field (CRF)}
\begin{itemize}
  \item Compared to Structured SVM, CRF has a probabilistic interpretation.
  \item We can draw samples in the output space.
  \pause
  \item How do we learn $w$? Maximum log likelihood, and regularization term: $\lambda \lVert w \rVert^2$
  \item Loss function:
  \begin{align*}
  l(w) &= - \frac{1}{N} \sum_{i=1}^{N} \log p(y^{(i)} | x^{(i)}) + \frac{1}{2} \lambda \lVert w \rVert^2 \\
  &= - \frac{1}{N} \sum_i \sum_t \sum_k w_k \psi_k(y^{(i)}_t, y^{(i)}_{t-1}) + \frac{1}{N} \sum_{i} \log Z(x^{(i)}) + \frac{1}{2}  \sum_k \lambda w_k^2
  \end{align*}
\end{itemize}
\end{frame}

\begin{frame}{Conditional random field (CRF)}
\begin{itemize}
  \item Loss function:
  \[
  l(w)= - \frac{1}{N} \sum_i \sum_t \sum_k w_k \psi_k(x^{(i)}, y^{(i)}_t, y^{(i)}_{t-1}) + \frac{1}{N} \sum_{i} \log Z(x^{(i)}) +\frac{1}{2} \sum_k  \lambda w_k^2
  \]
\item Gradient:
\begin{align}
\frac{\partial l(w)}{\partial w_k} &= -\frac{1}{N} \sum_i \sum_t \sum_k \psi_k(x^{(i)}, y^{(i)}_t, y^{(i)}_{t-1}) \\
& + \frac{1}{N} \sum_{i} \frac{\partial}{\partial w_k} \log \sum_{y' \in Y} \exp(\sum_t \sum_{k'} w_{k'} \psi_{k'}(x^{(i)}, y'_t, y'_{t-1})) + \sum_k \lambda w_k
\end{align}
\end{itemize}
\end{frame}

\begin{frame}{Conditional random field (CRF)}
\begin{itemize}
  \item What is $\frac{1}{N} \sum_i \sum_t \sum_k \psi_k(x^{(i)}, y^{(i)}_t, y^{(i)}_{t-1})$?
  \pause

  \item It is the expectation $\psi_k (x^{(i)}, y_t, y_{t-1})$ under the empirical distribution $\tilde{p}(x, y) = \frac{1}{N} \sum_i \mathbbm{1} [x = x^{(i)}] \mathbbm{1} [y = y^{(i)}]$.
\end{itemize}
\end{frame}


\begin{frame}{Conditional random field (CRF)}
\begin{itemize}
  \item What is $\frac{1}{N} \sum_i \frac{\partial}{\partial w_k} \log \sum_{y' \in Y} \exp(\sum_t \sum_{k'} w_{k'} \psi_{k'}(x^{(i)}, y_t', y_{t-1}'))$?
  \pause
  \begin{align}
  & \frac{1}{N} \sum_i \frac{\partial}{\partial w_k} \log \sum_{y' \in Y} \exp(\sum_t \sum_{k'} w_{k'} \psi_{k'}(x^{(i)}, y_t', y_{t-1}'))\\
  &= \frac{1}{N} \sum_i \left[\sum_{y' \in Y} \exp(\sum_t \sum_{k'} w_{k'} \psi_{k'}(x^{(i)}, y_t', y_{t-1}'))\right]^{-1}\\
  &\left[\sum_{y' \in Y} \exp(\sum_t \sum_{k'} w_{k'} \psi_{k'}(x^{(i)}, y_t^{(i)}, y_{t-1}^{(i)})) \sum_t \psi_k(x^{(i)}, y_t', y_{t-1}') \right] \\
  &= \frac{1}{N} \sum_i \sum_t \sum_{y'\in Y} p(y'_t, y_{t-1}' | x) \psi_k (x^{(i)}, y'_t, y'_{t-1})
  \end{align}
  \pause
  \item It is the expectation of $\psi_k (x^{(i)}, y'_t, y'_{t-1})$ under the model distribution $p(y'_t, y_{t-1}' | x)$.
\end{itemize}
\end{frame}


\begin{frame}{Conditional random field (CRF)}
\begin{itemize}
  \item To compute the gradient, we need to infer expectation under the model distribution $p(y|x)$.
  \pause
  \item Compare the learning algorithms: in structured SVM we need to compute the argmax, whereas in CRF we need to compute the model expectation.
  \pause
  \item Both problems are NP-hard for general graphs.
\end{itemize}
\end{frame}

\begin{frame}{CRF Inference}
\begin{itemize}
  \item In the linear chain structure, we can use the forward-backward algorithm for inference, similar to Viterbi.
  \pause
  \item Initiate $\alpha_j(1) = \exp (w^\top \psi(y_1=j, x_1))$
  \pause
  \item Recursion: $\alpha_j(t) = \sum_i \alpha_i (t-1) \exp(w^\top \psi(y_t=j, y_{t-1}=i, x_t))$
  \pause
  \item Result: $Z(x) = \sum_j \alpha_j(T)$
  \pause
  \item Similar for the backward direction.
  \pause
  \item Test time, again use Viterbi algorithm to infer argmax.
  \pause
  \item The inference algorithm can be generalized to belief propagation (BP) in a tree structure (exact inference).
  \pause
  \item In general graphs, we rely on approximate inference (e.g. loopy belief propagation).
\end{itemize}
\end{frame}

\begin{frame}{Examples}
\begin{itemize}
  \item POS tag
  Relationship between constituents, e.g. NP is likely to be followed by a VP.
  \pause

  \item Semantic segmentation
  
  Relationship between pixels, e.g. a grass pixel is likely to be next to another grass pixel, and a sky pixel is likely to be above a grass pixel.
  \pause

  \item Multi-label learning

  An image may contain multiple class labels, e.g. a bus is likely to co-occur with a car.
\end{itemize}
\end{frame}

\begin{frame}
{Conclusion}
Structured Prediction
\begin{itemize}
\item Extension to multi-class prediction
\item Structured SVM, CRF
\item Output space containing structure
\item Text and image applications
\end{itemize}
\end{frame}

\section{Decision Trees}

\begin{frame}
{Overview: Decision Trees}
\begin{itemize}
\item Our first inherently non-linear classifier: decision trees.
\item Ensemble methods: bagging and boosting.
\end{itemize}
\end{frame}

\section{Decision Trees}

\begin{frame}{Regression trees: Predicting basketball players' salaries}
    \begin{minipage}{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/basketball_decision_tree}
    \end{minipage}%
    \begin{minipage}{0.45\textwidth}
        \pause
        \includegraphics[height=\textwidth]{figures/basketball_regions}
    \end{minipage}
\end{frame}

\begin{frame}{Classification trees}
%\item Decision tree is a \textcolor{blue}{non-linear} classifier
\begin{center}
\includegraphics[height=0.6\textheight]{figures/dt-2d}
\end{center}
\begin{itemize}
\item Can we classify these points using a linear classifier?
    \pause
\item Partition the data into axis-aligned regions \textcolor{blue}{recursively} (on the board)
\end{itemize}
\note[item]{Let's consider the dataset shown in the figure. How should we classify the orange and the blue points?}
\note[item]{They are not linearly separable, but they belong to different regions. (draw regions)}
\note[item]{Let's partition the space recursively.}
\end{frame}

\begin{frame}
{Decision trees setup}
\begin{columns}
\begin{column}{0.5\textwidth}
\begin{center}
\includegraphics[height=0.7\textheight]{figures/generalTreeStructure}
\end{center}
\note<.>{Explain depth of a tree.}
\end{column}
\begin{column}{0.5\textwidth}
    \begin{itemize}[<+->]
\item We focus on \emph{binary} trees (as opposed to multiway trees where nodes can have more
than two children)
\item Each node contains a subset of data points
\item The data splits created by each node involve only a \emph{single} feature
\item For continuous variables, the splits are always of the form $x_{i}\le t$
\item For discrete variables, we partition values into two sets (not covered today)
\item Predictions are made in terminal nodes
\end{itemize}
\end{column}
\end{columns}
\let\thefootnote\relax\footnotetext{\tiny{From Criminisi et al. MSR-TR-2011-114, 28 October 2011.}}

\note[item]{Let's review some terminology for trees you've probably seen in an algorithm class. We'll consider a rooted tree, where we have a designated root node. The nodes that have degree one (incident to only one edge) are terminal/leaf nodes. The other nodes are called internal nodes.}
\note[item]{Specifically, we'll consider binary trees where each node has two children.}
\note[item]{What does each node represent? They correspond to one partition of the data. The root node contains all data; we partition the data in one node by branching to its descendants.}
\note[item]{The branching decision at each node involves a single feature.}
\end{frame}

   
\begin{frame}
    {Constructing the tree}
\begin{description}[<+->]
        \item[Goal] Find boxes $R_1, \ldots, R_J$ that minimize $\sum\limits_{j=1}^{J}\sum\limits_{i\in R_j} (y_i - \hat y_{R_j})^2$, subject to complexity constraints.
\item[Problem] Finding the optimal binary tree is computationally intractable.
\item[Solution] Greedy algorithm: starting from the root, and repeating until a stopping criterion is reached (\eg max depth), find the non-terminal node that results in the ``best'' split
    \begin{itemize}
        \item We only split regions defined by previous non-terminal nodes
    \end{itemize}
\item[Prediction] Our prediction is the mean value of a terminal node: $\hat y_{R_m} = \text{mean}(y_i \mid x_i \in R_m)$
    \begin{itemize}
\item A greedy algorithm is the one that make the best \textbf{local} decisions, without lookahead to evaluate their downstream consequences
    \item This procedure is not very likely to result in the globally optimal tree
    \end{itemize}
\end{description}
\note[item]{Given some stopping criteria, the next question is how do we find the split that will minimize our task loss in the end.}
\note[item]{In general, this is intractable.}
\note[item]{However, we can use a greedy algorithm to split each node in a locally optimal way, starting from the root.}
\end{frame}


\begin{frame}{Prediction in a Regression Tree}
    \begin{minipage}{0.3\textwidth}
        \includegraphics[height=0.5\textheight]{figures/regression_regions}
    \end{minipage}
    \begin{minipage}{0.6\textwidth}
    \includegraphics[height=0.6\textheight]{figures/regression_splits_and_surface}
    \end{minipage}
\end{frame}

\begin{frame}{Finding the Best Split Point}
\begin{itemize}[<+->]
\item We enumerate all features and all possible split points for each feature. There are infinitely many split points, but...
\item Suppose we are now considering splitting on the $j$-th feature $x_{j}$, and let $x_{j(1)},\ldots,x_{j(n)}$ be the sorted values of the $j$-th
feature.
\item We only need to consider split points between two adjacent values, and any split point in the interval $(x_{j(r)}, x_{(j(r+1)})$ will result in the same loss 
\item It is common to split half way between two adjacent values:
\begin{align}
s_{j}\in\left\{ \frac{1}{2}\left(x_{j(r)}+x_{j(r+1)}\right)\mid r=1,\ldots,n-1\right\} .
&& \text{$n-1$ splits}
\end{align}
\end{itemize}

\note[item]{Now that we have a measure of goodness for each split. How do we find that split? There are infinitely many split points for each continuous feature.}
\note[item]{But many split points will result in the same regions.}
\end{frame}

\begin{frame}
    {Decision Trees and Overfitting}
\begin{itemize}[<+->]
\item What will happen if we keep splitting the data into more and more regions?
\begin{itemize}
\item Every data point will be in its own region---\textcolor{red}{overfitting}.
\end{itemize}

\item When should we stop splitting? (Controlling the complexity of the hypothesis space)
\begin{itemize}
\item Limit total number of nodes.
\item Limit number of terminal nodes.
\item Limit tree depth.
\item Require minimum number of data points in a terminal node.
\pause
\item \textbf{Backward pruning} (the approach used in \textbf{CART}; Breiman et al 1984): 
\begin{enumerate}
\item Build a really big tree (e.g. until all regions have $\le5$ points).
\item \emph{Prune} the tree back greedily, potentially all the way to the root,
until validation performance starts decreasing.
\end{enumerate}
\end{itemize}
\end{itemize}
\note[item]{A common approach in practice is backward pruning, similar to backward feature selection. The advantage is that we get the global view once the entire tree is built, where as in the other approaches we might stop too early and miss an important split that only comes later.}
\end{frame}

\begin{frame}{Pruning: Example}
    \centering
    \includegraphics[height=0.8\textheight]{figures/pruning}
\end{frame}

\begin{frame}
{What Makes a Good Split for Classification?}

Our plan is to predict the \textbf{majority label} in each region.
\onslide<+->{
    \begin{simpleblock}{Which of the following splits is better?}
\begin{description}
\item[Split 1] $R_1: 8+ / 2- \qquad R_2: 2+ / 8-$
\item[Split 2] $R_1: 6+ / 4- \qquad R_2: 4+ / 6-$
\end{description}
\end{simpleblock}
}
\onslide<+->{
    \begin{simpleblock}{How about here?}
\begin{description}
\item[Split 1] $R_1: 8+ / 2- \qquad R_2: 2+ / 8-$
\item[Split 2] $R_1: 6+ / 4- \qquad R_2: 0+ / 10-$
\end{description}
\end{simpleblock}
}

\onslide<+->{
Intuition: we want to produce \emph{pure} nodes,
\ie nodes where most instances have the same class.
}
\note[item]{Split 1 is better because it has low error rate.}
\note[item]{It's a bit tricky now. Both split has the same error rate. Note that in Split 2, R2 would be a terminal node already, so we might prefer Split 2; however, R1 still needs more work.}
\end{frame}

\begin{frame}
{Misclassification error in a node}
\begin{itemize}
\item Let's consider the multiclass classification case: $\cy=\left\{ 1,2,\ldots,K\right\} $.

\item Let node $m$ represent region $R_{m}$, with $N_{m}$ observations
\pause
\item We denote the proportion of observations in $R_{m}$ with class $k$ by
\[
\hat{p}_{mk}=\frac{1}{N_{m}}\sum_{\left\{ i:x_{i}\in R_{m}\right\} }\ind{y_{i}=k}.
\]
\pause
\item We predict the majority class in node $m$: 
    \[ 
        k(m)=\argmax_{k}\hat{p}_{mk} 
    \]

%\item The misclassification rate in node $m$ is $1-\hat{p}_{mk(m)}$
\end{itemize}

\note[item]{Each node contains a subset of data.}
\end{frame}

\begin{frame}{Node Impurity Measures}
\begin{itemize}[<+->]
\item Three measures of \textbf{node impurity} for leaf node
$m$:
\begin{itemize}[<+->]
\item Misclassification error
\[
1 -  \hat{p}_{mk(m)}.
\]

\item The Gini index encourages $\hat{p}_{mk}$ to be close to 0 or 1
\[
\sum_{k=1}^{K}\hat{p}_{mk}(1-\hat{p}_{mk}).
\]

\item Entropy / Information gain
\[
-\sum_{k=1}^{K}\hat{p}_{mk}\log\hat{p}_{mk}.
\]
\end{itemize}
\item The Gini index and entropy are numerically similar to each other, and both work better in practice than the misclassification error.

\note[item]{We would like a metric that gives high scores to nodes containing examples from many different classes, and low scores to nodes with few or a single class of examples.}
\note[item]{We've seen that the error rate can indicate node purity. When is it minimized? When $p$ is 1, i.e., all examples are in the same class. Note that this is different from the Gini index that measure inequality you might learn in economics.}
\note[item]{But it's not a ``sensitive'' measure. It's often the case that we will have many splits that gives the same misclassification error. Gini index is another popular metric. For each class $k$ we compute the product of the proportion of class $k$, $\hat{p}_{mk}$ and the other classes $1-\hat{p}_{mk}$. When is it minimized? $p=0 / 1$.}
\note[item]{Finally, we can use Shannon entropy, which measure the uncertainty of the label in this node. It's minimized when all labels are the same---no uncertainty.}
\note[item]{Both Gini index and entropy are information-theoretic measures.}

\end{itemize}
\end{frame}

\begin{frame}{Impurity Measures for Binary Classification}

    \centering
    ($p$ is the relative frequency of class 1)
\begin{figure}
\includegraphics[height=0.5\textheight]{figures/impurityMeasureTwoClass}
\end{figure}
\vspace{-1em}
%Misclassification error is not strictly concave thus may not guarantee improvement over the parent node.
\note[item]{On the linear piece of misclassification error,
$w_1Q(p_1) + w_2Q(p_2) = Q(w_1p_1 + w_2p_2)$.}
\let\thefootnote\relax\footnotetext{\tiny{HTF Figure 9.3}}
\end{frame}

\begin{frame}{Quantifying the Impurity of a Split}
Scoring a potential split that produces the nodes $R_{L}$ and $R_{R}$:
\begin{itemize}
\item Suppose we have $N_{L}$ points in $R_{L}$ and $N_{R}$ points in
$R_{R}$.
\pause

\item Let $Q(R_{L})$ and $Q(R_{R})$ be the node impurity measures for each node.
\pause

\item We aim to find a split that minimizes the \emph{weighted average of node
impurities}:
\[
\frac{N_{L}Q(R_{L})+N_{R}Q(R_{R})}{N_L+N_R}
\]
\end{itemize}

\note[item]{Now that we've defined impurity measure for a single node, how do we use it to evaluate a split, which produces two nodes?}
\note[item]{The weight is just the proportion of examples in left and right nodes.}
\note[item]{$Q_1 = 1/4, Q_2=1/4, w_1 = 10/15=2/3, w_2 = 1/3.$}
\end{frame}

% \begin{frame}
% {Regression trees}
% \begin{itemize}
% \item Squared loss as the node impurity measure.
% %\item Everything else remains the same as classification trees.
% \end{itemize}

% \note[item]{How do we apply the same splitting strategy for regression problems? What needs to be changed?}
% \note[item]{First, instead of predicting the majority class, we predict the mean.}
% \note[item]{Impurity is easier---just squared loss, i.e. how much the values deviate from the mean.}
% \note[item]{So, to recap, to grow a basic decision tree, we pick an impurity measure and a stopping criterion, start from the root node, then recursively split until a stop criteria is reached. Next, let's consider some practical matters.}
% \end{frame}

%\subsection{Trees in General}
%\begin{frame}
%{\dis Categorical features}
%\begin{itemize}[<+->]
%\item For a categorical feature, we split its values into two groups.
%\item<.-> Given a set of categories of size $k$, how many distinct splits? (its power set)
%\item Finding the optimal split is \textcolor{red}{intractable} in general.
%\item Approximations
%\begin{description}[<.->][Numeric encoding]
%\item<+->[Numeric encoding] Randomly assign a number to each category
%\begin{itemize}
%\item Binary classification: proportion of class 0
%\item Regression: mean of targets of examples in the category, \ie  \textbf{mean encoding} 
%\end{itemize}
%\item[One-hot encoding] May grow imbalanced trees, \eg left-branching
%\item[Binary encoding] Robust to large cardinality
%\end{description}

%\item Statistical issues with categorical features
%\begin{itemize}[<.->]
%\item If a category has a very large number of categories, we can \al{overfit}.
%\item Extreme example: Row Number could lead to perfect classification with
%a single split.
%\end{itemize}
%\end{itemize}
%\note[item]{One unique characteristic of tree-based models is that they handle categorical features naturally.}
%\note[item]{$2^k$ subsets, divided by 2 for symmetry, minus 1 to remove the empty set}
%\note[item]{Approximation: we can either reduce it to the continuous case, or reduce the number of categories.
%To reduce it to the continuous case, we can randomly assign a number to each category. In binary classification, we can do it slightly smartly by assigning the proportion of negative examples in that category.
%To reduce the number of categories, we can use binary encoding, which creates multiple binary features out of one categorical features.}
%\note[item]{We should be careful with overfitting with there are a large number of categories though.}
%\end{frame}

\begin{frame}
{Discussion: Interpretability of Decision Trees}

    \centering
        \includegraphics[width=0.2\textwidth]{figures/basketball_decision_tree}
    \begin{itemize}[<+->]
    \item Trees are easier to visualize and explain than other classifiers (even linear regression)
\item Small trees are interpretable -- large trees, maybe not so much
%\item Approximate neural network decision boundaries to gain interpretability
%\begin{itemize}
%\item Wu M, Hughes M, Parbhoo S, Zazzi M, Roth V, Doshi-Velez F. \href{https://finale.seas.harvard.edu/files/finale/files/beyond_sparcity_tree_regularization_of_deep_01.pdf}{Beyond Sparsity: Tree Regularization of Deep Models for Interpretability}. Association for the Advancement of Artificial Intelligence (AAAI). 2018
%\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
{Discussion: Trees vs. Linear Models}
Trees may have to work hard to capture linear decision boundaries, but can easily capture certain nonlinear ones:
\begin{figure}
\includegraphics[height=0.6\textheight]{figures/treeVsLinear}
\end{figure}
\note[item]{Because it's hard of it to model the addition of two features.}
\end{frame}

\begin{frame}
{Discussion: Review}
\begin{simpleblock}{Decision trees are:}
\begin{itemize}
\item Non-linear: the decision boundary that results from splitting may end up being quite complicated
\item Non-metric: they do not rely on the geometry of the space (inner products or distances)
\item Non-parametric: they make no assumptions about the distribution of the data
\end{itemize}
\end{simpleblock}

\pause
\begin{simpleblock}{Additional pros:}
\begin{itemize}
\item Interpretable and simple to understand
\end{itemize}
\end{simpleblock}

\pause
\begin{simpleblock}{Cons:}
\begin{itemize}
\item Struggle to capture linear decision boundaries
\item They have high variance and tend to \al{overfit}: they are sensitive to small changes in the training data (The ensemble techniques we discuss next can mitigate these issues)
\end{itemize}
\end{simpleblock}

\note[item]{What are some pros and cons of trees?}
\note[item]{One big issue with trees is that they are very unstable, meaning that if the training data is changed slightly, our classifier will be quite different, and this makes the performance of a single tree not competitive.}
\end{frame}

\section{Bagging and Random Forests}
\subsection{Variance of an Estimator}
\begin{frame}
{Recap: Statistics and Point Estimators}
    \begin{itemize}[<+->]
\item We observe data $\cd=\left(x_{1},x_{2},\ldots,x_{n}\right)$ sampled i.i.d. from a parametric distribution $p(\cdot\mid \theta)$

\item A \textbf{statistic} $s=s(\cd)$ is any function of the data:
\begin{itemize}
\item E.g., sample mean, sample variance, histogram, empirical data distribution
\end{itemize}

\item A statistic $\hat{\theta}=\hat{\theta}(\cd)$ is a \textbf{point estimator}
of $\theta$ if $\hat{\theta}\approx\theta$
\end{itemize}

%\onslide<+->{
%\begin{block}{Review questions}
%In frequentist statistics,
%\begin{itemize}
%\item Is $\theta$ random?
%\item Is $\hat{\theta}$ random?
%\item Is the function $s(\cdot)$ random?
%\end{itemize}
%\end{block}
%}

\note[item]{When we say trees are not stable or have high variance, what do we mean by that precisely? Let's recall properties of point estimators we talked about in frequentist statistics.}
\note[item]{We call a statistic a point estimator if it approximates some unknown parameter.}
\note[item]{The function $s$ is not random, but we plug in random samples of data.}
\end{frame}

\begin{frame}
{Recap: Bias and Variance of an Estimator}
\begin{itemize}[<.->]
    \setlength\itemsep{2pt}
\item Statistics are random, so they have probability distributions.
\item The distribution of a statistic is called a \textbf{sampling distribution}. 
\item<+-> The standard deviation of the sampling distribution is called the \textbf{standard error}.
\item<+-> Some parameters of the sampling distribution we might be interested in:
\begin{description}
\item[Bias]  $\mbox{Bias}(\hat{\theta})\eqdef \ex\pb{\hat{\theta}}-\theta$.
\item[Variance] $\mbox{Var}(\hat{\theta})\eqdef \ex\pb{\hat{\theta}^{2}} - \ex^2\pb{\hat{\theta}}$.
\end{description}
%\item<+-> \dis Is bias and variance random?
%\begin{itemize}
    \note[item]{Neither bias nor variance depend on a specific sample $\cd_{n}$.
        We are \emph{taking expectation over $\cd$.}}
%\end{itemize}
\item<+-> Why does variance matter if an estimator is unbiased?
\pause
\begin{itemize}
    \item {$\hat{\theta}(\sD) = x_1$ is an unbiased estimator of the mean of a Gaussian, but would be farther away from $\theta$ than the sample mean.}
\end{itemize}
\end{itemize}
\note[item]{Next let's look at how we can reduce variance of an estimator.}
\end{frame}

\subsection{The Benefits of Averaging}
\begin{frame}
{Variance of a Mean}
\begin{itemize}
\item Let $\hat{\theta}(\sD)$ be an unbiased estimator with variance $\sigma^2$:
$\BE\pb{\hat{\theta}} = \theta$, $\mbox{Var}(\hat{\theta}) = \sigma^2$.
\item<+-> So far we have used a single statistic $\hat{\theta} = \hat{\theta}(\sD)$ to estimate $\theta$.
\item<+-> Its standard error is $\sqrt{\mbox{Var}(\hat{\theta})} = \sigma$
\end{itemize}

\onslide<+->{
\begin{itemize}
\item Consider a new estimator that takes the average of i.i.d. $\hat{\theta}_1, \ldots, \hat{\theta}_n$ where $\hat{\theta}_i = \hat{\theta}(\sD^{i})$.
    \pause
\item The average has the same expected value but smaller standard error (recall that $Var(cX) = c^2 Var(X)$, and that the $\hat{\theta}_i$-s are uncorrelated):
\begin{align}
\BE\pb{\frac{1}{n}\sum_{i=1}^n \hat{\theta}_i} = \theta \qquad
\mbox{Var}\pb{\frac{1}{n}\sum_{i=1}^n \hat{\theta}_i} = \frac{\sigma^2}{n}
\label{eqn:variance-of-sample-mean}
\end{align}
\end{itemize}
}
\note[item]{Let's say we want to estimate some parameter $\theta$.}
\note[item]{What is the standard error?}
\note[item]{Now let's consider a new estimator that takes the average of $n$ i.i.d. estimates, each computed from some data $\sD^i$.}
\note[item]{It will still be unbiased, but now the variance is reduced. Try to prove it yourself.}
\end{frame}

\begin{frame}{Averaging Independent Prediction Functions }
    \begin{itemize}[<+->]
\item Suppose we have $B$ independent training sets, all drawn from the same distribution ($\sD \sim p(\cdot\mid \theta)$).

\item Our learning algorithm gives us $B$ prediction functions: $\hat{f}_{1}(x),\hat{f}_{2}(x),\ldots,\hat{f}_{B}(x)$

\item We will define the average prediction function as:
\begin{align}
\hat{f}_{\text{avg}}\eqdef \frac{1}{B}\sum_{b=1}^{B}\hat{f}_{b}
\end{align}

%\item<+-> \dis What's random here?
%\begin{itemize}
    \note[item]{The $B$ independent training sets are random, which gives rise to
        variation among the $\hat{f}_{b}$'s.}
%\end{itemize}

%\item<+-> \think{Concept check}: What's the distribution of $\hat{f}$ called?
%What do we know about the distribution?
\end{itemize}

\note[item]{Concept check: sampling distribution. We don't know anything about it because we don't know the data generating distribution.}
\end{frame}

%
\begin{frame}{Averaging Reduces Variance of Predictions}
\begin{itemize}
\item The average prediction for $x_{0}$ is
\[
\hat{f}_{\text{avg}}(x_{0})=\frac{1}{B}\sum_{b=1}^{B}\hat{f}_{b}(x_{0}).
\]

\item $\hat{f}_{\text{avg}}(x_{0})$ and $\hat{f}_{b}(x_{0})$ have the
same expected value, but

\item $\hat{f}_{\text{avg}}(x_{0})$ has smaller variance:% (see \ref{eqn:variance-of-sample-mean}):
\begin{eqnarray*}
\mbox{\ensuremath{\var}}(\hat{f}_{\mbox{avg}}(x_{0})) = 
\frac{1}{B}\var\left(\hat{f}_{1}(x_{0})\right)
\end{eqnarray*}
\pause
\item \al{Problem}: in practice we don't have $B$ independent training sets!
\end{itemize}
\note[item]{The average prediction has the same expected value as the single prediction, but it has smaller variance.}
\note[item]{But are we done? Just train $n$ predictors and average them?}
\end{frame}

\subsection{Bootstrap}

\begin{frame}{The Bootstrap Sample}
\begin{simpleblock}
{How do we simulate multiple samples when we only have one?}
\begin{itemize}[<+->]
\item A \textbf{bootstrap sample} from $\cd_{n}=\left(x_{1},\ldots,x_{n}\right)$
is a sample of size $n$ drawn \emph{with replacement} from $\cd_{n}$

\item Some elements of $\cd_{n}$ 
will show up multiple times, and
some won't show up at all 
\end{itemize}
\end{simpleblock}

%\onslide<+->{\dis How similar are the bootstrap samples?}
\begin{itemize}[<+->]
\item Each $x_{i}$ has a probability of $(1-1/n)^{n}$ of not being
included in a given bootstrap sample

\item For large $n$,
\begin{align}
\left(1-\frac{1}{n}\right)^{n}\approx\frac{1}{e}\approx.368.
\end{align}

\item<.-> So we expect \textasciitilde 63.2\% of elements of $\cd_n$ will show
up at least once.
\end{itemize}

\note[item]{Now the question is how similar these bootstrap samples are. If they are pretty much the same, its distribution would be far from the actual data generating distribution.}
\note[item]{Because we are sampling with replacement, each element has $1/n$ probability to be selected during each draw. And after $n$ draws, the probability that it's still not selected is $1-1/n$ to the power of $n$.}
\note[item]{Actually $n$ doesn't have to be very large, with $n=1000$, we already reach this number here.}
\note[item]{Since about 60 percent examples shown in each bootstrap sample, they should be fairly different.}
\end{frame}
%
%
\begin{frame}{The Bootstrap Method}
\begin{definition}
A \textbf{bootstrap method } simulates $B$
independent samples from $P$ by taking $B$ bootstrap samples from
the sample $\cd_{n}$.

\end{definition}
\pause

\begin{itemize}
\item Given original data $\cd_{n}$, compute $B$ bootstrap samples $D_{n}^{1},\ldots,D_{n}^{B}$.

\pause
\item For each bootstrap sample, compute some function
\[
\phi(D_{n}^{1}),\ldots,\phi(D_{n}^{B})
\]
\pause

\item Use these values as though $D_{n}^{1},\ldots,D_{n}^{B}$ were
i.i.d. samples from $P$.

\item This often ends up being very close to what we'd get with independent samples from $P$!
\end{itemize}
\end{frame}
%
\begin{frame}{Independent Samples vs. Bootstrap Samples}
\begin{itemize}
\item Point estimator $\hat{\alpha}=\hat{\alpha}(\cd_{100})$ for samples
of size $100$, for a synthetic case where the data generating distribution is known

\item Histograms of $\hat{\alpha}$ based on
\begin{itemize}
    \item 1000 independent samples of size 100 (left), vs.
    \item 1000 bootstrap samples of size 100 (right)

\end{itemize}
\end{itemize}
\begin{figure}
\includegraphics[height=0.4\textheight]{figures/independentVsBootstrap}

\end{figure}

\let\thefootnote\relax\footnotetext{\tiny{Figure 5.10 from \emph{ISLR} (Springer, 2013) with permission from the authors: G. James, D. Witten,  T. Hastie and R. Tibshirani.}}
\end{frame}
%
%\begin{frame}{Side note: Bootstrap in Practice}
%We can use bootstrap to get error bars in a cheap way.
%\begin{itemize}
%\item Suppose we have an estimator $\hat{\theta}=\hat{\theta}(\cd_{n})$.

%\item To get error bars, we can compute the ``\emph{bootstrap variance}''. 

%\begin{itemize}

%\item Draw $B$ bootstrap samples.
%\item Compute sample variance of $\hat{\theta}(\cd_{n}^{1}),\ldots,\hat{\theta}(\cd_{n}^{B})$..

%\item Could report 
%\[
%\hat{\theta}(\cd_{n})\pm\sqrt{\mbox{Bootstrap Variance}}
%\]

%\end{itemize}
%\end{itemize}
%\end{frame}

\begin{frame}{Ensemble Methods}

{\textbf{Key ideas:}}

    \begin{itemize}[<+->]
    \item In general, \textbf{ensemble methods} combine multiple weak models into a single, more powerful model
    \item Averaging i.i.d. estimates reduces variance without changing bias
    \item We can use bootstrap to simulate multiple data samples and average them
    \item Parallel ensemble (\eg bagging): models are built independently
    \item Sequential ensemble (\eg boosting): models are built sequentially
        \begin{itemize}
        \item We try to find new learners that do well where previous learners fall short
        \end{itemize}
\end{itemize}

\end{frame}

\subsection{Bagging}
\begin{frame}{Bagging: Bootstrap Aggregation}
    \begin{itemize}[<+->]
\item We draw $B$ bootstrap samples $D^{1},\ldots,D^{B}$ from original data
$\cd$

\item Let $\hat{f}_{1},\hat{f}_{2},\ldots,\hat{f}_{B}$ be the prediction
functions resulting from training on $D^{1},\ldots,D^{B}$, respectively

\item The \textbf{bagged prediction function} is a \emph{combination }of
these:
\[
\hat{f}_{\text{avg}}(x)=\mbox{Combine}\left(\hat{f}_{1}(x),\hat{f}_{2}(x),\ldots,\hat{f}_{B}(x)\right)
\]

%\pause{}
%\item \dis How might we combine 
%\begin{itemize}
%\item prediction functions for regression?
%\item binary class predictions? 
%\item binary probability predictions?
%\item multiclass predictions? 
%\end{itemize}

    \end{itemize}
\end{frame}

\begin{frame}{Bagging: Bootstrap Aggregation}
    \begin{itemize}[<+->]

\item Bagging is a general method for variance reduction, but it is particularly useful for decision trees

\item For classification, averaging doesn't make sense; we can take a \textbf{majority vote} instead

\item Increasing the number of trees we use in bagging does not lead to overfitting

\item Is there a downside, compared to having a single decision tree?

\item Yes: if we have many trees, the bagged predictor is much less interpretable

\end{itemize}
\end{frame}

\begin{frame}{Aside: Out-of-Bag Error Estimation}
\begin{itemize}
\item Recall that each bagged predictor was trained on about 63\% of the data.
\item The remaining 37\% are called \textbf{out-of-bag (OOB)} observations.

\pause{}
\item For $i$th training point, let 
\[
S_{i}=\left\{ b\mid D^{b}\text{ does not contain }i\text{th point}\right\} 
\]

\item The \textbf{OOB prediction} on $x_{i}$ is
\[
\hat{f}_{\text{OOB}}(x_{i})=\frac{1}{\left|S_{i}\right|}\sum_{b\in S_{i}}\hat{f}_{b}(x_{i})
\]
\pause
\item The OOB error is a good estimate of the test error

\item Similar to cross validation error: both are computed
on the training set
\end{itemize}
\end{frame}

\begin{frame}{Applying Bagging to Classification Trees}
\begin{itemize}
\item Input space $\cx=\reals^{5}$ and output space $\cy=\left\{ -1,1\right\} $.
Sample size $n=30$.
\begin{columns}[t]

\column{.4\textwidth}

\includegraphics[width=1\columnwidth]{figures/baggedTrees}

\pause{}

\column{.6\textwidth}
\begin{itemize}[<+->]
\item Each bootstrap tree is quite different: different splitting variable at the root!

\item \textbf{High variance}: small perturbations of the training data lead to a high degree of model variability

\item Bagging helps most when the base learners are relatively unbiased but have high variance (exactly the case for
    decision trees)

\end{itemize}
\end{columns}

\end{itemize}
\let\thefootnote\relax\footnotetext{\tiny{From HTF Figure 8.9}}
\end{frame}
%

\subsection{Random Forests}
\begin{frame}{Motivating Random Forests: Correlated Prediction Functions}
Recall the motivating principle of bagging:
    \begin{itemize}[<+->]
\item For $\hat{\theta}_{1},\ldots,\hat{\theta}_{n}$ \emph{i.i.d.} with $\ex\pb{\hat{\theta}}=\theta$ and $\var\pb{\hat{\theta}}=\sigma^{2}$,
\[
\ex\left[\frac{1}{n}\sum_{i=1}^{n}\hat{\theta}_{i}\right]=\mu\qquad\var\left[\frac{1}{n}\sum_{i=1}^{n}\hat{\theta}_{i}\right]=\frac{\sigma^{2}}{n}.
\]

\item What if $\hat{\theta}$'s are correlated? 

%\item Suppose $\forall i\neq j$, $\text{Corr}(\hat{\theta}_{i},\hat{\theta}_{j})=\rho$ . Then
%\[
%\var\left[\frac{1}{n}\sum_{i=1}^{n}\hat{\theta}_{i}\right]=\rho\sigma^{2}+\frac{1-\rho}{n}\sigma^{2}.
%\]

\item For large $n$, the covariance term dominates, limiting the benefits of averaging
\item Bootstrap samples are 
\begin{itemize}
\item independent samples from the training set, but
\item \al{not} independent samples from $P_{\cx\times\cy}$
\end{itemize}

\item Can we reduce the dependence between $\hat{f}_{i}$'s?
\end{itemize}
\end{frame}

\begin{frame}{Random Forests}
\begin{simpleblock}{\textbf{Key idea}}
Use bagged decision trees, but modify the tree-growing procedure
to reduce the dependence between trees.
\end{simpleblock}

\begin{itemize}[<+->]
\item Build a collection of trees independently (in parallel), as before
\item When constructing each tree node, restrict choice of splitting
variable to a randomly chosen subset of features of size $m$
\begin{itemize}
    \item<.-> This prevents a situation where all trees are dominated by the same small number of strong features (and are therefore too similar to each other)
\end{itemize}
\item We typically choose $m\approx\sqrt{p}$, where $p$ is the number of
    features (or we can choose $m$ using cross validation)
\item If $m = p$, this is just bagging
\end{itemize}
\note[item]{When $m$ is too small, you might underfit. When $m$ is too large, it won't be effective at reducing dependence.}
\end{frame}
%
\begin{frame}{Random Forests: Effect of $m$} 
\begin{figure}
\includegraphics[height=0.7\textheight]{figures/randomForestVsBagging}
\end{figure}

\let\thefootnote\relax\footnotetext{\tiny{From \emph{An Introduction to Statistical Learning, with applications in R} (Springer, 2013) with permission from the authors: G. James, D. Witten,  T. Hastie and R. Tibshirani.}}
\end{frame}

\begin{frame}{Review}
\begin{itemize}[<+->]
\item The usual approach is to build very deep trees---low bias but \al{high variance}
\item Ensembling many models reduces variance
\begin{itemize}[<.->]
\item Motivation: Mean of i.i.d. estimates has smaller variance than single estimate
\end{itemize}
\item Use bootstrap to simulate many data samples from one dataset
\begin{itemize}[<.->]
\item $\implies$ Bagged decision trees
\end{itemize}
\item But bootstrap samples (and the induced models) are correlated
\item Ensembling works better when we combine a diverse set of
prediction functions
\begin{itemize}[<.->]
    \item $\implies$ Random forests: select a random subset of features for each decision tree
\end{itemize}
\end{itemize}
\end{frame}

\end{document}
